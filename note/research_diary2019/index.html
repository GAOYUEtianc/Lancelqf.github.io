<!DOCTYPE html>
<html class="no-js" lang="en-US" prefix="og: http://ogp.me/ns# fb: http://ogp.me/ns/fb#">
<head>
    <meta charset="utf-8">

    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
<meta name="description" content="">
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1">

<meta name="description" content="This is my research diary about intelligence in 2019.">
<meta name="keywords" content="Self, Research, ">

 
<meta property="og:type" content="article"/>
<meta property="og:description" content="This is my research diary about intelligence in 2019."/>
<meta property="og:title" content="Research Diary 2019 : lancelqf.github.io"/>
<meta property="og:site_name" content="Qingfeng Lan's blog"/>
<meta property="og:image" content="" />
<meta property="og:image:type" content="image/jpeg" />
<meta property="og:image:width" content="" />
<meta property="og:image:height" content="" />
<meta property="og:url" content="https://lancelqf.github.io/note/research_diary2019/">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2019-01-21"/>
<meta property="article:modified_time" content="2019-01-21"/>


<meta property="article:tag" content="Self">
<meta property="article:tag" content="Research">



    <base href="https://lancelqf.github.io">
    <title> Research Diary 2019 - lancelqf.github.io </title>
    <link rel="canonical" href="https://lancelqf.github.io/note/research_diary2019/">
    

    
<link rel="stylesheet" href="/static/css/style.css">
    <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon" />
    <link rel="apple-touch-icon" href="/apple-touch-icon.png" />
    
    <script type="text/javascript" async
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/MathJax.js?config=TeX-AMS-MML_HTMLorMML">

  MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$']],
    displayMath: [['$$','$$']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
  });

  MathJax.Hub.Queue(function() {
    
    
    
    var all = MathJax.Hub.getAllJax(), i;
    for(i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<style>
code.has-jax {
  font: inherit;
  font-size: 100%;
  background: inherit;
  border: inherit;
  color: #515151;
}
</style>

    <script>
      var _hmt = _hmt || [];
      (function() {
        var hm = document.createElement("script");
        hm.src = "https://hm.baidu.com/hm.js?2445edbea2f2541f3150c49cc08c381a";
        var s = document.getElementsByTagName("script")[0]; 
        s.parentNode.insertBefore(hm, s);
      })();
    </script>      

    
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-121043808-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-121043808-1');
    </script>
    
</head>
<body lang="en" itemscope itemtype="http://schema.org/Article">
<header id="header">
  <nav id="nav">
    <div id="title"><a href="/" class="blue"><font color="blue">Qingfeng Lan</font></a></div>
    <div><a href=mailto:lancelqf@gmail.com target="_blank" class="blue"><span class="icon-mail"></span></a></div>
  </nav>
  <nav id="nav">
    <ul id="mainnav">


  <li>
    <a href="/heart/">
    <span class="icon"> <i aria-hidden="true" class="icon-heart"></i></span>
    <span> Heart </span>
  </a>
  </li>

  <li>
    <a href="/note/">
        <span class="icon"> <i aria-hidden="true" class="icon-pencil"></i></span>
        <span> Note </span>
    </a>
  </li>

  <li>
  <a href="/tech/">
      <span class="icon"> <i aria-hidden="true" class="icon-gears"></i></span>
      <span> Tech </span>
  </a>
  </li>

  <li>
  <a href="/about">
      <span class="icon"> <i aria-hidden="true" class="icon-atom"></i></span>
      <span> About </span>
  </a>
  </li>

</ul>
    <ul id="social">
  <li id="share">
      <span class="title"> share </span>
      <div class="dropdown share">
          <ul class="social">
              <li> <a href="http://spf13.com/" target="_blank" title="spf13 is Steve Francis" class="facebook">spf13</a> </li>
              <li> <a href="http://nanshu.wang/" target="_blank" title="Dear Sister Nanshu" class="nanshu">Nanshu</a> </li>
              <li> <a href="http://spaces.facsci.ualberta.ca/rlai/" target="_blank" title="A RL&AI researh group at the University of Alberta" class="twitter">RLAI</a> </li>
              <li> <a href="https://deepmind.com/" target="_blank" title="An AI research company, part of the Alphabet group" class="googleplus">DeepMind</a> </li>
              <li> <a href="https://openai.com/" target="_blank" title="A non-profit AI research company" class="stumbleupon">OpenAI</a> </li>
              <li> <a href="http://whirl.cs.ox.ac.uk/" target="_blank" title="A ML research group at the University of Oxford" class="delicious">WhiRL</a> </li>             
          </ul>
          <span class="icon icon-bubbles"> </span> <span class="subcount"> </span>
      </div>
  </li>

  <li id="follow">
      <span class="title"> follow </span>
      <div class="dropdown follow">
          <ul class="social">
              <li> <a href="https://www.linkedin.com/in/qingfeng-lan-974685146" target="_blank" title="LinkedIn" class="linkedin">LinkedIn</a> </li>
              <li> <a href="https://github.com/qlan3" target="_blank" title="GitHub" class="github">GitHub</a> </li>
              <li> <a href="https://www.instagram.com/qingfeng_lan/" target="_blank" title="Instagram" class="facebook">Instagram</a> </li>
              <li> <a href="https://twitter.com/lancelan3" target="_blank" title="Twitter" class="twitter">Twitter</a> </li>
          </ul>
          <span class="icon icon-rocket"> </span> <span class="subcount"></span>
      </div>
  </li>
</ul>
  </nav>
</header>


<section id="main">
  <h1 itemprop="name" >Research Diary 2019</h1>
  

<aside id="meta">

    <div>
        <section id="datecount">
          <h4 id="date"> Mon Jan 21, 2019 </h4>
        </section>
        <ul id="tags">
          
            <li> <a href="https://lancelqf.github.io/tags/self">Self</a> </li>
          
            <li> <a href="https://lancelqf.github.io/tags/research">Research</a> </li>
          
        </ul>
    </div>
</aside>

<meta itemprop="wordCount" content="14495">
<meta itemprop="datePublished" content="2019-01-21">
<meta itemprop="url" content="https://lancelqf.github.io/note/research_diary2019/">

  <div>
        <article itemprop="articleBody" id="content">
           <p>This is my research diary about intelligence in 2019. I find writing research diary really interesting and it helps me to organize my thougths and discover new ideas! I just hope I have enough thougths to continue writing.</p>

<p></p>

<!--
- In current research, the codomain of reward function on $\mathbb{R}$, how about $\mathbb{R} ^ n$ where $n \in \mathbb{N}$?
- The state value is estimated by return (rewards sum after time $t$). However, we only have past and current rewards. How can we do this? How can this even be possible?
- pseudo reward / cummulant / nexting (see Adam White's paper and publications)
- Can we get rid of reward (especially manually designed reward)? —— nexing?!
-->

<!--
Because there is an infinite number of actions and (or) states to estimate the values for and hence value-based approaches are way too expensive computationally in the continuous space.

Can we and should we get rid of reward/value function? New goal lead to new value function which means we have to recompute almost everything.
reward function is a mapping from the history sequence to a real number.
state value function can be seen as a heuristic function used for heuristic search in RL problems.

deterministic or stochastic policy? the difference between deterministic and greedy method?
-->

<h1 id="reference">Reference</h1>

<ul>
<li><a href="http://incompleteideas.net/book/the-book.html">Reinforcement Learning: An Introduction. Richard S. Sutton and Andrew G. Barto, Second Edition, MIT Press, Cambridge, MA, 2018.</a></li>
<li><a href="http://aima.cs.berkeley.edu/">Artificial Intelligence: A Modern Approach. Stuart Russell and Peter Norvig, Third Edition, Prentice Hall, 2009.</a></li>
<li><a href="http://tutorial.transferlearning.xyz/">Transfer Learning Tutorial. Jindong Wang et al, 2018.</a></li>
<li><a href="https://en.wikipedia.org/wiki/">Wikipedia</a></li>
<li><a href="https://arxiv.org/abs/1811.12560v2">An Introduction to Deep Reinforcement Learning. Vincent François-Lavet, Peter Henderson, Riashat Islam, Marc G. Bellemare and Joelle Pineau, 2018.)</a></li>
<li><a href="https://spinningup.openai.com/en/latest/">Spinning Up in Deep RL</a></li>
<li><a href="https://lilianweng.github.io/lil-log/2018/04/08/policy-gradient-algorithms.html">Policy Gradient Algorithms. Lilian Weng, 2018.</a></li>
</ul>

<h1 id="january">January</h1>

<p><font size=6>01/21: What is intelligence? (1/2)</font></p>

<p>I'm going to talk about intelligence. To be specific, it is not just human intelligence or artificial intelligence but general intelligence that I want to talk about. Although it is really hard to find a proper and satisfying defnition for intelligence, it is still possible to name some traits of intelligence. Let's begin!</p>

<ul>
<li><p><strong>Adaptive</strong>: Intelligence is adaptive and flexible, easy to adjust itself. It should have a strong adaptivity that enables it to adapt to any environments. Image that a baby born on Mars or Moon. Although the gravaity is different than it on Earth, it won't take the baby too long to get used to the new environment. For human being or other forms of life, this ability is essential to their survival. So it is for an intelligent agent. Afterall, an agent that fails to survive cannot have enought time to do what it needs to do.<br />
But what does this abstract word, adaptive, really mean? Well, it is about making right decisions. But what is right? And how to make right descisions? Since I don't have a clear answer rigt now, it is better to leave it for future elaboration.</p></li>

<li><p><strong>Robust</strong>: An intelligent agent should be complex enough to cope with possible errors. There are basic two ways: the self-correcting way and the fault-tolerant way. The self-correcting way is more active while the fault-tolerant way is more passive. Robustness overlaps with self-optimizing (see below).</p></li>

<li><p><strong>Self-optimizing</strong>: While making actions is about changing the outside state, self-optimizing is about changing the inner state of the agent itself. Self-improving is a simliar word. Evolving has a broader meaning. It emphasizes changing, not just improving or optimizing. Updating knowledge system and optimizing action outputs are two good examples. In a word, the agent improves itself in order to better achieve the goal.</p></li>
</ul>

<p>Note that the three traits mentioned above are strongly connect with each other.</p>

<p><font size=6>01/22: What is intelligence? (2/2)</font></p>

<ul>
<li><p><strong>Efficient</strong>: In real world, we often need to solve optimzation problems, such as finding the shortest way to school, makeing as much as money given fixed time and being sucessful as young as possible. In short, we want to achieve the goal efficiently. Similarly, a good intelligent agent should be efficient enough to realize the final goal. For example, energy saving and less time wasting. However, this may not always be the case because I am talking about efficiency in the sense of archieving goal. The goal can also be related to inefficiency, such as riding a bike as slow as you can. So the efficiency I mean, is based on the goal totally. Planning and optimization play important roles here.</p></li>

<li><p><strong>Learning ability</strong>: Although by hardcoding knowledge into an agent's &quot;brain&quot; may solve most practical problems, it requres a great amount of manual work. Moreover, it is not flexiable——hardcoding means it is not easy to update current knowledge thus making self-optimizing impossible.<br />
Instead of trying to produce a programme to simulate the adult mind, why not rather try to produce one which simulates the child's? (Alan Turing) Children have incredible learning ability and they learn fast. They are born with the ability to learn which is encoded in DNA during the long period of evolution.<br />
Life long learning, few shot learning, multi-task learning, multi-agent Learning, meta learning and transfer learning can all be included in this part. Next, I will introduce two aspects of learning:</p>

<ul>
<li><strong>learning knowledge</strong>: This is the most obvious aspect of learning. It is a process of extracting features, patterns and rules from raw data generated by the interaction between the agent and the environment (direct experience) or from existing data (indirect experience). It's similar to discovery of physics laws. Supervise and unsupervise learning are two good examples. Moreover, after discovering new knowledge, it is also necessary to embed it into current knowledge system and to make connections from old knowledge to new knowledge. By doing this, an agent can accumulate knowledge as time goes on and wield knowledge in a more powerful way. Finally, the agent is able to construct a model of the world.<br />
Is learning knowledge an application of funtion approximation? Can any knowledge be expressed as functions? To answer these questions, we need to define knowledge itself. Generally, what we call knowledge can be divided into two subgroups: the knowledge to environment and meta knowledge (the knowledge of knowledge itself). I leave it for future consideration.</li>
<li><strong>learning how to learn</strong> (also known as <strong>meta learning</strong>): Learning is not only about learning knowledge but also learning how to learn. I call this kind of knowledge as meta knowledge (the knowledge of knowledge itself). It is more abstract than the knowledge of environment, similar to (but not the same as) the relation between physics and mathematics (This is still not a good comparsion, but the best I can find right now.). Things like how to discover knowledge more quickly and when/where/how to apply it are two examples.</li>
</ul></li>

<li><p><strong>Others</strong>: creativity, imagination... The meaning of these words are vague and abstract. I don't know how to translate them into more concrete ones. Maybe they are just manifestations of searching, exploring and combining old things into new things.</p></li>
</ul>

<p><font size=6>01/23: What's inside in an intelligent agent?</font></p>

<p>The traits of intelligence are like softwares in a computer. So what hardwares do we need in order to support the normal running of softwares? In my opinion, these subsystems are necessary for an intelligent agent:</p>

<ul>
<li><strong>Sensor</strong>: Any things that preceives the environment counts as a sensor. An agent may have many sensors. A sensor can be defined as a mapping from environment to signals. For example, an ear transforms sound waves to acoustical signals and an eye transforms photons of lights to images. The sensors create what the agent can &quot;see&quot;. The signals that sensors output are the only information source of the environment. Anything that cannnot be perceived by sensors does not exist with regard to the agent. Note that what sensors perceive may be wrong, i.e. the signals produced by sensors do not reflect the real world accurately.</li>
<li><strong>Actor</strong>: An actor is anything that an agent can control to influence the outside environment. It receives commands from the decision make and then act according to the commands appropriately. The actions  include simple reflex action, model-based action and goal-based action. They can be atomic, factored or structured.</li>
<li><strong>Decision maker</strong>: It is the brain of an agent and also the most important subsystem for decision making. The inputs are the signals received by sensors. And it outputs commands which are excuted by actors. To analyze it meticulously, I divide it into several parts.

<ul>
<li><strong>Knowledge base</strong>: A knowledge base is where an agent stores knowledge.</li>
<li><strong>Thinker</strong>: It is the core of descision maker and also employs most computation resources. It makes a series of decisions given access to knowledge base and signals from sensors. If necessary, it also updates knowledge in the knowledge base or even create a new one from scatch. It is the basis of intelligence. All other parts only exist to support it. I don't have too many ideas about it and may spend the rest of my life to figure out how to make one.</li>
</ul></li>
</ul>

<p>Can we separate the knowledge base with the thinker totally? Yes! One good example is McCarthy's Advice Taker in which the rules and the reasoning component are separated. But is it good? I'm not sure. It's possible that these subsystems are strongly connected or even merged together without clear boundaries. If the knowledge base and the planner are separated completely, it may take more time to access necessary knowledge before making a proper decision. Thus, a distributed knowledge base may be more efficient. For example, knowledge in neural networks is represented by weight matrice between layers. The decision making process consists of repetitions of acessing to knowledge (weights) and computation (compute activation functions, do matrix multiplication, etc.). The two procedures are followed one after the other in the forward process. Further thoughts and emprical experiments are still needed.</p>

<p><font size=6>01/24: Knowledge base</font></p>

<p>Before considering the knowledge base, it is better starting from more basic questions.</p>

<ul>
<li><p>What avaliable knowledge representations we can use?</p>

<ul>
<li><strong>Human language (written language)</strong>: Almost any knowledge can be expressed as language. It has inner structures, mapping from symbols to real world objects or even from symbols to symbols, full of bootstraping and self-reference. It evolves as time goes.<br /></li>
<li><strong>Vector</strong>: Methods such as word embedding, map words or phrases to vectors of real numbers. Since we can encode human language into a sequence of bytes (i.e. numbers) under some standard rules, any knowledge represented by human language can also be represented by a bunch of numbers (i.e. vectors). Afterall, they are all symbols.</li>
<li><strong>Graph</strong>: Knowledge graphs are human understandable and easy for computers to operate. They are also very powerful to represent knowledge. Maybe it can be a good interlanguage between human and computers. I don't know too much about this. <strong>Need future study</strong>.</li>
</ul></li>

<li><p>What is a good knowledge representation?<br />
In fact, whether a knowledge representation is good or bad really depends on the agent itself and the goal. I should say that any knowledge that helps the agent to achieve the goal is a good one. Although it is hard to give a clear answer, typically, a good knowledge representation is:</p>

<ul>
<li><strong>powerful</strong>: It should be able to express enough knowledge.</li>
<li><strong>compact</strong>: The representation should have high information density which is space-saving.</li>
<li><strong>easy to operate</strong>: It should be easy to handle and operate by the agent.<br /></li>
</ul>

<p>Note that a good representation is not necessary human readable although it will be really helpful.</p></li>

<li><p>Now let's consider knowledge base. A good knowledge base is:</p>

<ul>
<li><strong>easy to retrive</strong>: It should be easy (less time consuming) for agent to find necessary knowledge.</li>
<li><strong>easy to update</strong>: It should be easy to add, delete and change knowledge in the database.</li>
<li><strong>space-saving</strong>: If the knowledge is too small, then there will not be enough space to store all knowledge the agent discovers. However, if it is too big, it takes a large amount of time to access. Both cases are not favourable. The best way to resolve this conflict is to find a space-saving way to store knowledge. Some compression methods can be applied if it is necessary, suitable and appropriate.</li>
</ul></li>

<li><p>How do we (i.e. human being) store and retrieve memory? No idea. <strong>Need future study</strong>.</p></li>
</ul>

<p><font size=6>01/25: Reward function (1/3)</font></p>

<p>How to guide an agent to achieve the final goal. Use reward! (But do we really need it? Is it necessary?)</p>

<ul>
<li>Are binary ((0,1),(-1, +1)) or integer reward functions as powerful as any real number reward functions?<br />
&quot;Powerful&quot; is not very clear. What I mean is that can any policy generated using real number reward functions be generated by binary reward functions? I think the answer should be &quot;Yes&quot;. In general value iteration, we first do policy evaluation by computing state values or state-action values based on reward sequence we receive. After that, policy improvement is applied to find better policies. To be more specific, we need the information about how to choose the best action given state values or state-action values. How do we do this? We compare numbers and then choose the action with max state-action value. In fact, it doesn't matter whether these numbers are real numbers or integers. What matters is the order. And integer is an ordered field which should be enough. However, real reward function has a big advantage. Since real number field is dense, we can always find a real number between two different real numbers. For example, asumme that we assign $1$ to $Q(s,a _ 1)$ and $2$ to $Q(s,a _ 2)$. Then here comes $Q(s,a _ 3)$. And we find that $a _ 3$ is better than $a _ 1$ but worse than $a _ 2$, i.e. $1=Q(s,a _ 1) &lt; Q(s,a _ 3) &lt; Q(s,a _ 2)=2$. We can always find a real number between 1 and 2 for $Q(s,a _ 3)$ but not an integer. If we insist to use integers, we must adjust the value of $Q(s,a _ 1)$ and $Q(s,a _ 2)$ as well. So apparently, real number reward functions are easier to use. But binary and integer reward functions have their own merits——they are easier to design. Which one is more sample efficient? Which one helps the agent learn faster? <strong>Need future study</strong>.</li>
</ul>

<hr />

<p><font size=6>01/26: Reward function (2/3)</font></p>

<p>What are we talking about when we talk about rewards?</p>

<p>What are rewards? Where do they come from? Look around and then you will find that there are many things that we call reward, such as money paid by your boss, delicious food, compliment from people who respect you. But are they really reward? Definitely not! They are just what they are. We create rewards ourselves. Rewards come from our brain, especially the chemical reward system in the brain. Through the chemical reward system, the brain guides the body, drive it for food and away from danger. In short, rewards are interpretations of current state (both the environment and agent itself). They are a measure how good or better the current state is. And how close are we from the final goal. Typically, we use a real number to indicate this. The larger the reward value is, the better the current state is and the closer we are from the final goal. For example, if the goal is finding the shortest path from $cityA$ to $cityB$, the reward can be the inverse of path length from current position to $cityB$.</p>

<p>Note that the reward I'm talking about here is not same as the reward descriped in Reinforcement Learning. The reward in RL defines the goal, indicates what is good or bad in an immediate sense. In this context, however, it indicates what is good or bad in the long run. So for consistency, I call the reward that I mentioned before <strong>cumulation</strong>, denoted as $C_t$ at time $t$. And the new reward $R_t$ is defined as a function of cumulation, i.e. $R _ t \doteq C _ t - C _ {t-1}$. In another view, the cumulation is a cumulative sum of the reward sequence: $C _ t = R _ 1 + R _ 2 + \cdots + R _ t = \sum _ { k = 1 } ^ { t } R _ { k }$. The new reward is same as the reward in RL. But the cumulaiton is not return. As a reminder, the return is defined as $G _ { t } \doteq R _ { t + 1 } + R _ { t + 2 } + R _ { t + 3 } + \cdots + R _ { T }$. The return is a accumulation of future rewards while cumulation is a accumulation of past rewards. They are two aspects.</p>

<p>Similar to discounted return, we can also add discount factor to cumulation: $C _ t = R _ 1 + \gamma R _ 2 + \cdots + {\gamma} ^ {t-1} R _ t = \sum _ { k = 1 } ^ { t } \gamma ^ { k - 1 } R _ { k }$ and $R _ t = \frac{C _ t - C _ {t-1}}{{\gamma} ^ {t-1}}$.</p>

<p><font size=6>01/28: Reward function (3/3)</font></p>

<p>As I elaborated in last research diary, &quot;Rewards are interpretations of current state (both the environment and agent itself). They are a measure how good or better the current state is. And how close are we from the final goal.&quot; So $R _ t$ should be a function of environment state $s _ {env}$ and agent state $s _ {agent}$, i.e. $R _ t: S _ E \times S _ A arrow \mathbb{R}$ where $S _ E$ is the environment state space and $S _ A$ is agent state space. In current typical RL setting, $R _ t$ is only determined by the environment. The agent state is not considered. This is because first, the reward design remains to be a difficult problem. Second, most RL agents are too simple. There are no such thing as agent states!</p>

<p>Instead of designning reward functions manually, can we design some methods to learn reward functions? For example, can we apply neural networks to approximate the true rewards (if they do exist) through bootstrapping? I think there should be some previous works (It's so obvous afterall!), or maybe someone tries this idea before but it didn't work. <strong>Need future study</strong>.</p>

<p><center><img src="/media/agent–environment_interaction.jpg" alt="The agent–environment interaction in a MDP." height="70%" width = "70%" /></center></p>

<p><font size=6>01/29: State</font></p>

<p>Define the state space $S$ to be $S \doteq S _ E \times S _ A$ where $S _ E$ is the environment state space and $S _ A$ is the agent state space. For every state $s$, it can written as $s=(s _ e, s _ a)$. If we use a vector to represent a state $s$, then each dimension of this vector is a feature of this state (e.g. $s=(f _ 1, f _ 2, \cdots, f _ n)$).</p>

<p>One advantage of representing states using vectors is that vectors are simple, comprehensible and easy to handle—they are just a bunch of features. However, since there is no internal structure inside a vector, it is not convenient to represent complex states or encode structural information in it, such as feature interaction (e.g. ${f _ 1 ^ 2} f _ 2$). One way to fix this is adding new features (e.g. $f _ {n+1} = {f _ 1 ^ 2} f _ 2$). But this usually leads to the curse of dimensionality. Thus we need a more compact but still powerful enough representation. And they should be manageable by agents (e.g. computers).</p>

<p>How about structural representations, such as trees and graphs? Are they good representations for states? Well, I'm not sure. Take the graph for example. If nodes are objects or features, edges that connect nodes can be the relations between objects or features. They are more powerful than vectors, since ordered disconnected nodes are equivalent to vectors. Moreover, nodes may have inner structure. In fact, we can even include part of graph inside a node! However, since graphs are more complex than vectors, they are also more difficult to handle with. And what is the edge that connects two nodes in the sense of mathematics? <strong>Need future study</strong>.</p>

<p><font size=6>01/30: Action (1/2)</font></p>

<p>Let $A$ be the action space. Then an action can be viewed as a mappings from a distribution of state space to another distribution of state space, i.e. $A: p(S) arrow p(S)$. Note that this includes the mappings from one state to another state if all probability is concentrate on a particular state, i.e. $p(s)=1$ for state $s$ while $p(\cdot)=0$ for other states. For example, $a(p _ 1)=p _ 2$ where $p _ 1(s)=\mathcal{N}(0,1)$ and $p _ 2(s)=\mathcal{N}(1,4)$.</p>

<p>If an action only influences the variation between two distributions, we have following properties:</p>

<ul>
<li>$\Delta {p _ a} \doteq p _ 1 - p _ 2$ is only determined by action $a$ where $a(p _ 1)=p _ 2$. To be specific, $p _ 2 \doteq a(p _ 1)=p _ 1 + \Delta {p _ a}$.<br /></li>
<li>$\int_{s \in S} \Delta {p _ a} ds = 0$</li>
</ul>

<p>Note that this is still not well-defined. For example, $\Delta {p _ a}(s) = s$ for $s \in S=[-1,1]$. $p _ 1$ is a uniform distribution, defined as $p _ 1(s) = 1/2$. Then $p _ 2 \doteq a(p _ 1) = p _ 1 + \Delta {p _ a} = s + 1/2$. Easy to see $p _ 2 (s=-1) = -1/2$. However, $p _ 2$ is a probability density function, so $p _ 2 (s) \geq 0$ for all $s \in S$.</p>

<p>A possible way to fix this: we move the distribution function by a constant $c$, i.e. $p(s) =p(s) + c$ such that $\int_{s \in S} {max(p(s)+c, 0)} ds = 1$. The $p _ 2$ in the above example then becomes $p _ 2(s) = s + \sqrt{2} - 1$ for $s \in  [1 - \sqrt{2}, 1]$ . This is not a good solution since the computation of $c$ is not easy.</p>

<p>However, if we use sampling, a negative probability won't be a big problem. We can always sample from states that have a positive probability and discard the state we sampled out whose probability is negative.</p>

<h1 id="february">February</h1>

<p><font size=6>02/01: Action (2/2)</font></p>

<p>Recall that $p(s ^ { \prime } | s , a)$ is the probability of transition from state $s$ to $s ^ {\prime}$ under action $a$. We also have:
$$p _ 2(s ^ {\prime})= \sum_{s \in S} p(s ^ { \prime } | s , a) p _ 1(s) \quad \textrm{where} \quad a(p _ 1) = p _ 2$$</p>

<p>However, we can not recover state transition probability $p(s ^ { \prime } | s , a)$ simply from $p _ 1(s)$ and $p _ 2(s)$ since there are more unknown values than equations. This means that we can not have all information from $a(\cdot)$. But is $a(\cdot)$ useful enough? I don't know actually...</p>

<p>We divide all actions based on state space $S$ roughly in three categories:</p>

<ul>
<li>Atomic actions: They have no internal structures and are indivisible. They are the lowest level actions an agent can act. They are simple but also important. They are the points in the action space $A$. For completeness, we also add the identity action $a _ I$ in $A$ where $a _ I(p)=p$ for all $p$.</li>
<li>Composite actions: They are the actions in $A ^ n$ where $n \in \mathbb{N}$. For each composite action $a$, it is defined as $a \doteq a _ 1 \circ \dots \circ a _ n$ where $a _ i \in A$ for all $i \in {1, 2, \dots ,n}$.</li>
<li>Total actions: They are all possible actions that th agent can use, denote as $A ^ \infty$.  For each action $a \in {A ^ \infty}$, it is defined as $a \doteq a _ 1 \circ a _ 2 \circ \dots \circ \dots = \circ _ {i=1} ^ {\infty} a _ i$ where $a _ i \in A$ for all $i \in \mathbb{N}$.</li>
</ul>

<hr />

<p><font size=6>02/02: Hierarchical states and actions</font></p>

<p>Hierarchical states and actions are quite useful in planning and solving other problems. One way of constructing hierarchical states is abstraction (any other ways?). Then the natural questions are: what is abstraction and how to do that?</p>

<p>Well, abstraction is about leaving out unnecessary details, focusing on only general characteristics. Instances that have the same abstraction should share something in common. Based on this idea, we define a <strong>abstract state set</strong>:
 $$AS_f \doteq [x \in \Omega | f(x) \in P ]$$
 (There is a problem with the display of {}, I use [] instead.)
 where $\Omega$ is a set of instances; $f$ is a transform function that the abstract state set bonds to; $P$ is a set that represents some properties. For example, $\Omega = \mathbb{R} ^ n$; $f: \mathbb{R} ^ n arrow \mathbb{R} ^ m$ is a projection function: $f(x) = y$ where $x=(v _ 1,\cdots,v _ n)$ and $x=(v _ 1,\cdots,v _ m)$, $m&lt;n$; $P=[x=(v _ 1,\cdots,v _ m) \in \mathbb{R} ^ m | v _ 1 = \cdots = v _ m ]$. Under this setting, $AS_f = [x=(v _ 1,\cdots,v _ n) \in \mathbb{R} ^ n | v _ 1 = \cdots = v _ m ]$.</p>

<p>Tile coding and state aggregation are two ways of defining abstract state set. Perhaps a more specific and simple way of defining abstract state set is clustering. The idea behind is very simple: instances that near each other under some metric should belong to a same abstract class. This can be viewed as doing clustering in the instance space.</p>

<p>Moreover, we can continue to define the second level abstract state set based on the (first) abstract state set:
  $$AS _ {f} ^ {2} \doteq [x \in \Omega | f(x) \in P ]$$
  where $\Omega$ is a subset of $AS _ {g}$. Similarly, we can define $n$-th level abstract state set $AS _ {f} ^ {n}$.</p>

<p>Similar to how we define the action on state space, we can define the $n$-th level action on the $n$-th abstract state set.</p>

<p><font size=6>02/04: Model (1/2): What is a model?</font></p>

<p>What is a model? According to Collins dictionary, &quot;A model of an object is a physical representation that shows what it looks like or how it works. The model is often smaller than the object it represents.&quot; In this definition, there are two important aspects:</p>

<ol>
<li>The model is often smaller than the object it represents.</li>
<li>The model help us understand how the object looks like or how it works. Furthermore, the model helps us predict the behavior of the object.</li>
</ol>

<p>What mathematical tools should we use to represent a model? Well, right now, for discrete process, we have deterministic finite automaton (DFA), nondeterministic finite automaton (NFA) and Markov decision process (MDP). In fact, they are quite similar. For continuous case, I don't know too much yet. <strong>Need future study</strong>. So for the rest, I focus only on discrete case.</p>

<p>Based on MDP, I define a simliar but also different mathematical framework, called Markov transition process (MTP):</p>

<blockquote>
<p>A MTP is a 5-tuple ($S, A, P _ a, S _ 0, F$), where<br />
  1. $S$ is a finite set of states.<br />
  2. $A$ is a finite set of actions.<br />
  3. $P _ {a}$ is the state transition probability defined as $a(p _ 1)=p _ 2$ for each action $a \in A$. If $p _ 1 (s) = 1$ for an action $a$, then we have $p _ 2 (s ^ {\prime}) = P _ {a} (s _ {t+1}=s ^ {\prime} \mid s _ {t}=s) = \operatorname{Pr}(s _ {t+1}=s ^ {\prime} \mid s _ {t}=s, a _ t = a)$ which is the probability that action $a$ in state $s$ at time $t$ will lead to state $s ^ {\prime}$ at time $t+1$.<br />
  4. $S _ 0 \subset S$ is a set of start states.<br />
  5. $F \subset S$ is a set of goal states.</p>
</blockquote>

<p>Note that the hierarchical states and actions also fits the definition of MTP simply by replacing $Q$ and $A _ s$ whith $n$-th level abstract state set and $n$-th level action set, respectively.</p>

<p>Also, sometimes we need to model a world without the interference of the agent. We can include this case by adding null action denoted as $\emptyset$. Thus $P _ {\emptyset}$ the state transition probability for null action.</p>

<p><font size=6>02/05: Model (2/2): Rethink</font></p>

<p>Things I need to consider further:</p>

<ol>
<li>The above model is still too simple. Afterall, the world is not Markov.</li>
<li>The world is changing and evolving eternally. How can the above model deal with a changing world? How to update?</li>
<li>The world is so complex and there are so many possible states and actions. Thus it is impossible to store all state transition probability. Possible solutions：

<ol>
<li>Use function approximation.</li>
<li>Only save the most useful/important/relevent/latest state transition probability. Introduce forgetting mechanism.</li>
<li>Instead of saving a probability distribution, we save several next states with large probability.</li>
</ol></li>
<li>Sometimes we don't know the true state transition probability but only transition samples. Thus we need to calculate the estimated state transtion probability from samples. However, since all estimation is inaccurate and induce variance. How to resolve this problem without getting more samples?</li>
<li>How can we apply transfer learning or one-shot learning to new states transitions?</li>
<li>How do I predict the world? Do I have a world model in my brain? Do I store a set of states somewhere in my brain? If so, how does the brain represent a state?</li>
<li>How much information can we store in our brain in all our lives?</li>
</ol>

<p><font size=6>02/07: Policy (1/2)</font></p>

<p>According the definition in Sutton's book, a policy defines the learning agent's way of behaving at a given time; a mapping from perceived states of the environment to actions to be taken when in those states.</p>

<p>In RL setting, typically there are two ways to learn a policy:<br />
1. Value-based methods: These methods (such as state value methods and state-action value methods) learn the action values and then select actions based on the estimated action values.<br />
2. Policy gradient methods: Instead of estimating action values, these methods learn a parameterized policy that select actions without a value function. A value function may still be used to learn the policy parameter, but is not required for action selection (Sutton, 2018).</p>

<p>My question is that are there any other approaches to generate a policy? For example, rule-based methods? Futhermore, do we have to use a reward fucntion? Is it really necessary?</p>

<p>Instead, can we learn a metric $M$ that measures how &quot;close&quot; is the current state to the goal states? If so, then what supervised information can we use to correct a wrong measurement? But is this method really different with value-based methods? Maybe not.</p>

<p><font size=6>02/08: Policy (2/2)</font></p>

<p>Anyway, let's try! Define the metric over (abstract) state set: $M: S \times S arrow [0,+\infty)$. Since it is a metric, for $x,y,z \in S$, the following conditions are satisfied:</p>

<blockquote>
<ol>
<li>non-negativity: $M(x,y) \geq 0$</li>
<li>identity of indiscernibles: $M(x, y)=0 rightarrow x=y$</li>
<li>symmetry: $M(x,y)=M(y,x)$</li>
<li>Delta inequality: $M(x,z) \leq M(x,y) + M(y,z)$</li>
</ol>
</blockquote>

<p>We also have $M(F, F)=0$. Our goal is to reach a state $s$ such that $s= \arg \min _ {s \in S} M(s,F)$. Denote $\operatorname{Pr}(s _ {t+1}=s ^ {\prime} \mid s _ {t}=s, a _ t = a)$ as $\operatorname{Pr}(s ^ {\prime} \mid s, a)$ for short. Then for greedy action selection:
$$\pi (s)=\arg \min _ {a \in A} \Sigma _ {s ^ {\prime} \in S} \operatorname{Pr}(s ^ {\prime} \mid s, a) M(s ^ {\prime}, F)$$
or more generally:
$$\pi (s)=\arg \min _ {a \in A} \Sigma _ {s ^ {\prime} \in S} p _ 2(s ^ {\prime}) M(s ^ {\prime}, F)  \text{ where } a(p _ 1)=p _ 2$$
Compared with value-based policy,
$$
\pi (s)=\arg \max _ {a \in A} Q(s, a) = \arg \max _ {a \in A} \Sigma _ {s ^ {\prime} \in S} \operatorname{Pr}(s ^ {\prime} \mid s, a) (R + \gamma V(s ^ {\prime}))
$$
notice that if $M(s ^ {\prime}, F) \propto -(R + \gamma V(s ^ {\prime}))$, the two ways of generating policy are exact same. So it seems that we still need use something similar to reward or state value as a measurement of the agent's performance... Maybe this can be combined with some heuristic search algorithm or used as a better initilization for state values.</p>

<hr />

<p><font size=6>02/11: Transfer Learning: Introduction (1)</font></p>

<p>I've run out of my ideas. Time to learn new things. I start with transfer learning. After that, I'd probably continue with online learning, life-long learning and meta learning.</p>

<p>For transfer learning, there are two important concepts:
- Domain: A domain is consists of data and data distribution. To be specific, there are two domains that we care in transfer learning: Source Domain ($D _ s$) and Target Domain ($D _ t$).
- Task: Task is the goal of learning. It consists of two parts: the label spaces ($Y$) and the corresponding learning functions ($f(\cdot)$).</p>

<p>Now, we give a formal definition of transfer learning:
&gt; Given the source domain $D _ s = [\mathbf{x} _ {i}, y _ {i}] _ {i=1} ^ {n}$ with labels and target domain $D _ t = [\mathbf{x} _ {j}] _ {j=1} ^ {m}$ without labels. The data distributions are different, i.e. $P(\mathbf{x} _ {s}) \neq P(\mathbf{x} _ {t})$. The goal of transfer learning is using knowledge learned form source domain to predict the labels in target domain.</p>

<p>The core of transfer learning is to find shared knowledge between two domains and apply it properly. Knowledge is learned in source domain and then applied in target domain. In a word, it is about searching for the invariable (or similarity) in changing domains and then apply it.</p>

<p><font size=6>02/12: Transfer Learning: Metrics (2)</font></p>

<p>The next question is how to measure the similarity? Well, we need a metric. But what is a good metric for it? The bad news is that there is no certain answer for all transfer learning problems. Different metrics are useful in dfferent ways and in different problems. The good news is that we have many metrics in the arsenal:</p>

<ul>
<li><p>Distance:</p>

<ul>
<li>Euclidean distance: $d _ {Euclidean} = \sqrt {(\mathrm{x} - \mathrm{y}) ^ {\top} (\mathrm{x} - \mathrm{y})}$</li>
<li>Minkowski distance： $d _ {Minkowski} = (| \mathbf { x } - \mathbf { y } | ^ { p }) ^ { 1 / p }$. When $p=1$, it's Manhattan distance; when $p=2$, it's Euclidean distance.</li>
<li>Mahalanobis distance: $d _ {Mahalanobis} = \sqrt { ( \mathrm { x } - \mathrm { y } ) ^ { \top } \Sigma ^ { - 1 } ( \mathrm { x } - \mathrm { y } ) }$. $\Sigma$ is the covariance of distribution. When $\Sigma = \mathbf{I}$, it's Euclidean distance.</li>
</ul></li>

<li><p>Similarity:</p>

<ul>
<li>Cosine similarity: $\cos ( \mathbf { x } , \mathbf { y } ) = \frac { \mathbf { x } \cdot \mathbf { y } } { | \mathbf { x } | \cdot | \mathbf { y } | } \in [0, 1]$.</li>
<li>Mutual information: the mutual information of two discrete random variables $X$ and $Y$ can be defined as: $I ( X ; Y ) = \sum _ { x \in X } \sum _ { y \in Y } p ( x , y ) \log \frac { p ( x , y ) } { p ( x ) p ( y ) }$. For continous random variables, we have $\mathrm { I } ( X ; Y ) = \int _ {Y} \int _ {X} p ( x , y ) \log ( \frac { p ( x , y ) } { p ( x ) p ( y ) }) dx dy$.</li>
<li>Pearson coefficient: For two random variables $X$ and $Y$, $\rho _ {X, Y} = \frac{\operatorname{Cov}(X, Y)} {\sigma _ {X} \sigma _ {Y}} \in [−1, 1]$.</li>
<li>Jaccard coefficient: For two sets $X$ and $Y$, the Jaccard coefficient is defined as: $J = \frac { X \cap Y } { X \cup Y }$. Furthermore, Jaccard distance = $1 − J$.</li>
</ul></li>

<li><p>Divergence:</p>

<ul>
<li>Kullback–Leibler (KL) divergence: For two distributions $P(x)$ and $Q(x)$, $D _ {KL}(P | Q)=\sum _ {x \in X} P(x) \log \frac{P(x)}{Q(x)}$. A continous version: $D _ {KL}(P | Q) = \int _ {- \infty} ^ {\infty} p ( x ) \log (\frac {p(x)}{q(x)}) dx$. Notice that $D _ {KL}(P | Q) \neq D _ {KL}(Q | P)$.</li>
<li>Jensen–Shannon divergence: Denote $M = \frac { 1 } { 2 } ( P + Q )$, $JSD(P|Q)=\frac{1}{2} D _ {KL}(P|M)+\frac{1}{2}D _ {KL}(Q|M)$.</li>
</ul></li>

<li><p>Maximum mean discrepancy (MMD):<br />
$$MMD(X , Y) = \sqrt{ || \sum _ { i = 1 } ^ { n _ { 1 } } \phi ( \mathbf { x } _ { i } ) - \sum _ { j = 1 } ^ { n _ { 2 } } \phi ( \mathbf { y } _ { j } ) || _ { \mathcal { H } } ^ { 2 } }$$
where $\phi(\cdot)$ is a mapping from orignal vector space to Reproducing Kernel Hilbert Space (RKHS).
<!--- Principal Angle: --></p></li>

<li><p>A-distance: We first train a classifier $h$ to distinguish whether instances are from source domain or target domain. We then define A-distance to be:
$$\mathcal{A}(\mathcal{D} _ {s},\mathcal{D} _ {t})=2(1 - 2 err(h))$$
where $err(h)$ is the hinge loss of this classifier $h$.</p></li>

<li><p>Hilbert-Schmidt Independence Criterion: It can be used to check the dependence of two sets of data:
$$HSIC (X, Y) = \operatorname{trace}(HXHY)$$
where $X$ and $Y$ are kernel form of two datasets.</p></li>

<li><p>Wasserstein Distance: Let ($M, d$) be a metric space for which every probability measure on $M$ is a Radon measure (a so-called Radon space). For $p\geq 1$, let $P _ {p}(M)$ denote the collection of all probability measures $\mu$ on $M$ with finite $p ^ {\text{th}}$ moment for some $x _ {0}$ in $M$,
$$\int _ { M } d(x, x _ {0}) ^ { p } \mathrm { d } \mu ( x ) &lt; + \infty$$
Then the $p ^ {\text{th}}$ Wasserstein distance between two probability measures $\mu$ and $\nu$ in $P _ {p}(M)$ is defined as:
$$W _ { p } ( \mu , \nu ) : = ( \inf _ { \gamma \in \Gamma ( \mu , \nu ) } \int _ { M \times M } d ( x , y ) ^ { p } \mathrm { d } \gamma ( x , y ) ) ^ { 1 / p }$$
where $\Gamma ( \mu , \nu )$ denote the collection of all measures $M \times M$ with marginals $\mu$ and $\nu$ on the first and second factors repectively. The Wasserstein metric may be equivalently defined by:
$$W _ { p } ( \mu , \nu ) ^ { p } = \inf \mathbb{E} [ d ( X , Y ) ^ { p }]$$
where $\mathbb{E}[Z]$ denotes the expected value of a random variable $Z$ and the infimum is taken over all joint distributions of the random variables $X$ and $Y$ with marginals $\mu$ and $\nu$ respectively.<br />
It seems that Wasserstein distance is quite popular these days, especially in GAN and domain adaptation.</p></li>
</ul>

<p><font size=6>02/13: Transfer Learning: Methods (3)</font></p>

<ul>
<li><p>Instance based Transfer Learning:</p>

<ul>
<li>By reusing samples in source domain and weighting them properly, we can transfer the learned knowledge from source domain and target domain. A naive way of weights setting is setting them to be $\frac{P(\mathbf{x} _ {t})}{P(\mathbf{x} _ {s})}$. This is similar to what we do to importance sampling ratio in RL. <em>TrAdaboost</em> introduces the idea of Adaboost to transfer learning: increasing the weights of samples that improve the performance of transfer learning and decreasing the weights of samples that harm the performance. Can we apply similar ideas for importance sampling ratio? <strong>Need future study</strong>.</li>
<li>Although instance based transfer learning has a good theoretical guarantee, it only applies to problems when the difference of $P(\mathbf{x} _ {s})$ and $P(\mathbf{x} _ {t})$ is small. The knowledge transfered in this method is not abstract enought.</li>
</ul></li>

<li><p>Feature based Transfer Learning: This method assumes that some features are shared by source domain and target domain. By feature transformation, it minimizes the distance between two sets of features or maps all features into a same feature space. The core question is how to do feature transformation and how to learn the mapping?</p></li>

<li><p>Parameter/Model based Transfer Learning: This method assumes that some model parameters can be shared by source domain and target domain. Through parameters sharing, knowledge learned from one domain can be transfered to another domain. Most algorithms developed in this approach are connected with neural networks strongly.</p></li>

<li><p>Relation Based Transfer Learning: In this method, logic is applied to learn the relations between objects in source domain. Then these relations are reused in target domain. This may be the most abstract method for transfer learning. Also, it is hard. So there are not too many papers.</p></li>
</ul>

<p><font size=6>02/14: Transfer Learning: Deep transfer learning (4)</font></p>

<p>Deep neural networks can learn features from the raw data end-to-end, including general features and specific features. Then the next question is how to decide which features of layers to transfer? There is no theoretical answer for this question. However, the experiments shows that:
- Features represented by weights in the first few layers are more general.
- By fine-tuning the neural networks, we can improve the peformance significantly.
- Transfer weights are better than random weights.
- By transfer weights in layers can accelerate learning.</p>

<p>Finetune can accelerate learning and save training time. However, it can not overcome the difference between training data and test training. By adding some adaptation layers, it can be overcomed to some extent. Furthermore, an additional loss is added to account for domain adaptation loss.</p>

<p><font size=6>02/15: Transfer Learning: Frontier (5)</font></p>

<ul>
<li>Artifical intelligence and human knowledge: Through the long history, we human being accumulate a large amount of knowledge. How to transfer these knowledge to agents? How to encode our knowledge into agent? Yes, we can always find a way to encode some particular knowledge into agent. However, the final goal is to find a general way to encode all human knowledge. And this is really hard. The most difficult part is to find a suitable knowledge representation that is understandale to human being as well as intelligent agents.</li>
<li>Transitive transfer learning: Although there may only be minor similarity between two domains, everything in this world is connected in some way. And If we can find a similarity chain that connects two different domains, we may find a way to transfer knowledge from one end to another end along this chain. This is the basic idea of transitive transfer learning. Surprisingly, it <a href="https://www.aaai.org/ocs/index.php/AAAI/AAAI17/paper/viewPaper/14446">works</a>!</li>
<li>Learning to Transfer: The goal of learning to transfer is to learn when to transfer, what to transfer and how to transfer. The general method includes two parts: learn experiences from previous cases and then apply them on new problems. Its main goal is to learn transfer learning experience. Formally,we define transfer learning experience:
$$E = (S,T,a,l)$$
where $S$ and $T$ are source and target domain, respectively. $a$ is a transfer learning algorithm. $l$ shows the performance improvement compared to learning performance without transfer learning. What is a useful transfer learning experience then? Everything that helps improve performance!</li>
<li>Online transfer learning: There are not many works.</li>
<li>Transfer reinforcement learning: It is a combination of transfer learning and reinforcement learning.</li>
</ul>

<hr />

<p><font size=6>02/18: Deep Reinforcement Learning: Value-based methods for deep RL (1)</font></p>

<p><center><img src="/media/general_schema_of_DRL.jpg" alt="General schema of deep RL methods" height="80%" width="80%" /></center></p>

<ul>
<li><p>Q-learning:</p>

<ul>
<li>The Q-learning algorithm uses Bellman equation to get the unique solution $Q ^ {*} (s, a)$:
$$Q ^ { * } ( s , a ) = ( \mathcal { B } Q ^ { * } ) ( s , a )$$
where $\mathcal{B}$ is the Bellman operator mapping any function $K : \mathcal { S } \times \mathcal { A } arrow \mathbb { R }$ into another function $\mathcal { S } \times \mathcal { A } arrow \mathbb { R }$, defined as follows:
$$( \mathcal { B } K ) ( s , a ) = \sum _ { s ^ { \prime } \in S } P ( s , a , s ^ { \prime } ) ( R ( s , a , s ^ { \prime } ) + \gamma \max _ { a ^ { \prime } \in \mathcal { A } } K ( s ^ { \prime } , a ^ { \prime } ) )$$</li>
<li>By <strong>Banach's theorem</strong>, the fixed point of the Bellman operator $\mathcal{B}$ exists since it is a contraction mapping. So Q-learning algorithm can learn the optimal Q-value function. In practice, one general proof of convergence to the optimal value function is available (Watkins and Dayan, 1992) under the conditions that:</li>
<li>the state-action pairs are represented discretely,</li>
<li>all actions are repeatedly sampled in all states (which ensures sufficient exploration, hence not requiring access to the transition model).</li>
</ul></li>

<li><p>Fitted Q-learning:</p>

<ul>
<li>In ﬁtted Q-learning, the algorithm starts with some random initialization of the Q-values $Q(s, a; \theta _ {0})$ where $\theta _ {0}$ refers to the initial parameters. Then, an approximation of the Q-values at the $k$th iteration $Q(s, a; \theta _ {k})$ is updated towards the target value:
$$Y _ { k } ^ { Q } = r + \gamma \max _ { a ^ { \prime } \in \mathcal { A } } Q ( s ^ { \prime } , a ^ { \prime } ; \theta _ { k } )$$
where $\theta _ {k}$ refers to some parameters that define the Q-values at the $k$th iteration.</li>
<li>In neural ﬁtted Q-learning (NFQ), the Q-values are parameterized with a neural network $Q(s, a; \theta _ {k})$ where the parameters $\theta _ {k}$ are updated by stochastic gradient descent by minimizing the square loss:
$$\mathrm { L } _ { D Q N } = ( Q ( s , a ; \theta _ { k } ) - Y _ { k } ^ { Q } ) ^ { 2 }$$
The parameters are updated as:
$$\theta _ { k + 1 } = \theta _ { k } + \alpha ( Y _ { k } ^ { Q } - Q ( s , a ; \theta _ { k } ) ) \nabla _ { \theta _ { k } } Q ( s , a ; \theta _ { k } )$$
where $\alpha$ is a scalar step size called the learning rate. Notice that when updating the weights, one also changes the target. Also, Q-values tend to be overestimated due to the max operator.</li>
</ul></li>
</ul>

<p><font size=6>02/19: Deep Reinforcement Learning: Value-based methods for deep RL (2)</font></p>

<ul>
<li><p>Deep Q-networks:
<center><img src="/media/DQN.jpg" alt="Sketch of the DQN algorithm" height="80%" width="80%" /></center></p>

<ul>
<li>Similar to fitted Q-learning, the target Q-network is:
$$Y _ { k } ^ { Q } = r + \gamma \max _ { a ^ { \prime } \in \mathcal { A } } Q ( s ^ { \prime } , a ^ { \prime } ; \theta _ {k} ^ {-})$$
where $\theta _ { k } ^ { - }$ are updated only every $C \in \mathbb { N }$ iterations with the following assignment: $\theta _ { k } ^ { - } = \theta _ { k }$. This prevents the instabilities to propagate quickly and it reduces the risk of divergence as the target values $Y _ { k } ^ { Q }$ are kept fixed for $C$ iterations.</li>
<li>In an online setting, the replay memory keeps all information for the last $N _ {replay} \in \mathbb{N}$ time steps. The updates are then made on a set of tuples $<s,a,r,s ^ {\prime} >$ (called mini-batch) selected randomly within the replay memory. This allows for updates that cover a wide range of the state-action space. In addition, one mini-batch update has less variance compared to a single tuple update.</li>
<li>To keep the target values in a reasonable scale and to ensure proper learning in practice, rewards are clipped between -1 and +1. Clipping the rewards limits the scale of the error derivatives and makes it easier to use the same learning rate across multiple games (however, it introduces a bias).</li>
</ul></li>

<li><p>Double DQN:</p>

<ul>
<li>The max operation in Q-learning uses the same values both to select and to evaluate an action. This makes it more likely to select overestimated values in case of inaccuracies or noise, resulting in overoptimistic value estimates. Therefore, the DQN algorithm induces an upward bias. The double estimator method uses two estimates for each variable, which allows for the selection of an estimator and its value to be uncoupled (Hasselt, 2010). This allows for the removal of the positive bias in estimating the action values.</li>
<li>In Double DQN, or DDQN, the target value $Y _ {k} ^ {Q}$ is replaced by:
$$Y _ {k} ^ {DDQN} = r + \gamma Q ( s ^ { \prime } , \underset { a \in \mathcal { A } } { \operatorname { argmax } } Q ( s ^ { \prime } , a ; \theta _ { k } ) ; \theta _ { k } ^ { - })$$
which leads to less overestimation of the Q-learning values, as well as improved stability, hence improved performance. Note that the policy is still chosen according to the values obtained by the current weights $\theta$.</li>
<li>How about triple DQN or $N$ DQN? Possbile research project.</li>
</ul></li>
</ul>

<p><font size=6>02/20: Deep Reinforcement Learning: Value-based methods for deep RL (3)</font></p>

<ul>
<li><p>Dueling network architecture:</p>

<ul>
<li>The dueling network architecture decouples the value and advantage function $A ^ {\pi}(s, a)$. The Q-value function is given by:
$$Q ( s , a ; \theta ^ { ( 1 ) } , \theta ^ { ( 2 ) } , \theta ^ { ( 3 ) } ) = V ( s ; \theta ^ { ( 1 ) } , \theta ^ { ( 3 ) } )+ ( A ( s , a ; \theta ^ { ( 1 ) } , \theta ^ { ( 2 ) } ) - \max _ { a ^ { \prime } \in \mathcal { A } } A ( s , a ^ { \prime } ; \theta ^ { ( 1 ) } , \theta ^ { ( 2 ) } ) )$$
(Question: why not just Q=V+A?)<br />
For $a ^ { * } = \operatorname { argmax } _ { a ^ { \prime } \in \mathcal { A } } Q ( s , a ^ { \prime } ; \theta ^ { ( 1 ) } , \theta ^ { ( 2 ) } , \theta ^ { ( 3 ) } )$, we have $Q ( s , a ^ { * } ; \theta ^ { ( 1 ) } , \theta ^ { ( 2 ) } , \theta ^ { ( 3 ) } ) = V ( s ; \theta ^ { ( 1 ) } , \theta ^ { ( 3 ) } )$.</li>
<li>The structure of dueling network:
<center><img src="/media/dueling_network.jpg" alt="Illustration of the dueling network architecture" height="80%" width="80%" /></center>
<br /></li>
</ul>

<p>The stream $V( s ; \theta ^ { ( 1 ) } , \theta ^ { ( 3 ) })$ provides an estimate of the value function, while the other stream produces an estimate of the advantage function. The learning update is done as in DQN and it is only the structure of the neural network that is modiﬁed.</p>

<ul>
<li>A slightly different approach is preferred in practice because it increases the stability of the optimization:
$$Q ( s , a ; \theta ^ { ( 1 ) } , \theta ^ { ( 2 ) } , \theta ^ { ( 3 ) } ) = V ( s ; \theta ^ { ( 1 ) } , \theta ^ { ( 3 ) } ) + ( A ( s , a ; \theta ^ { ( 1 ) } , \theta ^ { ( 2 ) } ) - \frac { 1 } { | \mathcal { A } | } \sum _ { a ^ { \prime } \in \mathcal { A } } A ( s , a ^ { \prime } ; \theta ^ { ( 1 ) } , \theta ^ { ( 2 ) } ) )$$
In that case, the advantages only need to change as fast as the mean, which appears to work better in practice.</li>
</ul></li>

<li><p>Distributional DQN:</p>

<ul>
<li>Another approach is to aim for a richer representation through a value distribution, i.e. the distribution of possible cumulative returns. This value distribution provides more complete information of the intrinsic randomness of the rewards and transitions of the agent within its environment (note that it is not a measure of the agent's uncertainty about the environment).</li>

<li><p>The value distribution $Z ^ {\pi}$ is a mapping from state-action pairs to <strong>distributions of returns</strong> when following policy $\pi$. It has an expectation equal to $Q ^ {\pi}$:
$$Q ^ { \pi } ( s , a ) = \mathbb { E } [ Z ^ { \pi } ( s , a ) ]$$
This random return is also described by a recursive equation:<br />
$$Z ^ { \pi } ( s , a ) = R ( s , a , S ^ { \prime } ) + \gamma Z ^ { \pi } ( S ^ { \prime } , A ^ { \prime } )$$
where we use capital letters to emphasize the random nature of the next state-action pair $(S ^ {\prime}, A ^ {\prime})$ and $A ^ { \prime } \sim \pi ( \cdot | S ^ { \prime } )$.</p></li>

<li><p>This approach has two main advantages: 1. It is possible to implement <strong>risk-aware behavior</strong>. 2. It leads to more performant learning in practice. The distributional perspective naturally provides a richer set of training signals than a scalar value function $Q(s,a)$. These training signals that are not a priori necessary for optimizing the expected return are known as <strong>auxiliary tasks</strong> and lead to an improved learning.</p></li>
</ul></li>
</ul>

<p><font size=6>02/21: Deep Reinforcement Learning: Value-based methods for deep RL (4)</font></p>

<ul>
<li><p>Multi-step learning:</p>

<ul>
<li>Non-bootstrapping methods learn directly from returns (Monte Carlo) and an intermediate solution is to use a multi-step target. Such a variant in the case of DQN can be obtained by using the n-step target value given by:
$$Y _ { k } ^ { Q , n } = \sum _ { t = 0 } ^ { n - 1 } \gamma ^ { t } r _ { t } + \gamma ^ { n } \max _ { a ^ { \prime } \in A } Q ( s _ { n } , a ^ { \prime } ; \theta _ { k } )$$
where $( s _ { 0 } , a _ { 0 } , r _ { 0 } , \cdots , s _ { n - 1 } , a _ { n - 1 } , r _ { n - 1 } , s _ { n })$ is any trajectory of $n+1$ time steps with $s = s _ 0$ and $a = a _ 0$.</li>
<li>A combination of different multi-steps targets can also be used:
$$Y _ { k } ^ { Q , n } = \sum _ { i = 0 } ^ { n - 1 } \lambda _ { i } ( \sum _ { t = 0 } ^ { i } \gamma ^ { t } r _ { t } + \gamma ^ { i + 1 } \max _ { a ^ { \prime } \in A } Q ( s _ { i + 1 } , a ^ { \prime } ; \theta _ { k } ) )$$
with $\sum _ { i = 0 } ^ { n - 1 } \lambda _ { i } = 1$. In the method called TD($\lambda$), $n \rightarrow \infty$ and $\lambda _ { i }$ follow a geometric law: $\lambda _ { i } \propto \lambda ^ { i }$ where $0 \leq \lambda \leq 1$.</li>
</ul></li>

<li><p>Bootstrapping:</p>

<ul>
<li>Disadvantage: using pure bootstrapping methods (such as in DQN) are prone to instabilities when combined with function approximation because they make recursive use of their own value estimate at the next time-step. Methods that rely less on bootstrapping can propagate information more quickly from delayed rewards as they learn directly from returns.</li>
<li>Advantage: using value bootstrap allows learning from off-policy samples. With bootstrapping, most algorithms learn faster.</li>
</ul></li>
</ul>

<p><font size=6>02/22: Possible RL Project: Maxmin learning with approximation</font></p>

<p>The basic idea for this project is combining double learning with approximation. As mentioned in Rich's book, many algorithms involve a maximization operation which leads to maximization bias. By using double learning technique, the selection of an estimator and its value is uncoupled. Thus, we can remove the maximization bias to some extent. For example, for double Q-learning, the update is</p>

<p>$$Q _ { 1 } ( S _ { t } , A _ { t } ) \leftarrow Q _ { 1 } ( S _ { t } , A _ { t } ) + \alpha [ R _ { t + 1 } + \gamma Q _ { 2 } ( S _ { t + 1 } , \underset { a } { \arg \max } Q _ { 1 } ( S _ { t + 1 } , a ) ) - Q _ { 1 } ( S _ { t } , A _ { t } ) ]$$</p>

<p>A comparison of Q-learning and double Q-learning on a simple episodic MDP:</p>

<p><center><img src="/media/Double_Q-learning.jpg" alt="Double Q-learning" height="80%" width="80%" /></center></p>

<p>As shown in the above picture, double Q-learning is less affected by maximization bias. However, there is still a small gap between the optimal and what double Q-learning really archieves. Why is that? I think the main reason is that although the selection of an estimator and its value is uncoupled, the maximization operation still induces bias.</p>

<p>To address this problem, I come up with a new algorithm called maxmin Q-learning. Compared with double Q-learning, there are two important difference.</p>

<ul>
<li>There are $n$ Q functions $Q _ 1, \cdots, Q _ n$where $n \in \mathbb{N}$ instead of just 2.</li>
<li>The update is consists of two steps:

<ul>
<li>First, compute $Q _ {min} (s, a) = min(Q _ 1(s, a), \cdots, Q _ n(s, a))$.</li>
<li>With probability $\frac{1}{n}$, update one of Q functions, say $Q _ 1$: $Q _ { 1 } ( S _ { t } , A _ { t } ) \leftarrow Q _ { 1 } ( S _ { t } , A _ { t } ) + \alpha [ R _ { t + 1 } + \gamma Q _ { min } ( S _ { t + 1 } , \underset { a } { \arg \max } Q _ { min } ( S _ { t + 1 } , a ) ) - Q _ { 1 } ( S _ { t } , A _ { t } ) ]$</li>
</ul></li>
</ul>

<p>For tabular case, maxmin Q-learning is better than double Q-learning on a simple episodic MDP. And it archieves the optimal!</p>

<p><center><img src="/media/maxmin_Q-learning.png" alt="Maxmin Q-learning" height="80%" width="80%" /></center></p>

<p>I want to further test this algorithm in function approximation case. To be specific, I want to check if it can still mitigate maximization bias and compare the performance of between maxmin Q-learning and double Q-learning.</p>

<p>Moreover, since we have $n$ Q functions, can we apply ideas in Adaboost to get a better (with less bias) estimation of Q value?</p>

<hr />

<p><font size=6>02/25: Deep Reinforcement Learning: Policy gradient methods (1)</font></p>

<p>Stochastic Policy Gradient</p>

<ul>
<li><p>The expected return of a stochastic policy $\pi$ starting from a given state $s _ 0$:
$$V ^ { \pi } \left( s _ { 0 } \right) = \int _ { \mathcal { S } } \rho ^ { \pi } ( s ) \int _ { \mathcal { A } } \pi ( s , a ) R ^ { \prime } ( s , a ) da ds$$
where $R^{\prime}(s, a)=\int_{s^{\prime} \in \mathcal{S}} \operatorname{Pr}\left(s, a, s^{\prime}\right) R\left(s, a, s^{\prime}\right)$ and $\rho \pi (s)$ is the discounted state distribution defined as:
$$\rho ^ { \pi } ( s ) = \sum _ { t = 0 } ^ { \infty } \gamma ^ { t } \operatorname {Pr} ( s _ { t } = s | s _ { 0 } , \pi )$$</p></li>

<li><p>For a differentiable policy $\pi _ w$, the fundamental result underlying these algorithms is the policy gradient theorem:
$$\nabla _ { w } V ^ { \pi _ { w } } \left( s _ { 0 } \right) = \int _ { \mathcal { S } } \rho ^ { \pi _ { w } } ( s ) \int _ { \mathcal { A } } \nabla _ { w } \pi _ { w } ( s , a ) Q ^ { \pi _ { w } } ( s , a ) da ds$$
This result allows us to adapt the policy parameters from experience. This result is particularly interesting since the policy gradient does not depend on the gradient of the state distribution (even though one might have expected it to). The REINFORCE algorithm is a simple example.</p></li>

<li><p>Policy gradient methods should include two steps:</p>

<ul>
<li>Policy evaluation: estimates $Q ^ { \pi _ { w } }$.</li>
<li>Policy improvement: it takes a gradient step to optimize the policy $\pi _ w(s, a)$ with respect to the value function estimation. Intuitively, the policy improvement step increases the probability of the actions proportionally to their expected return.</li>
</ul></li>

<li><p>How to obtain an estimate of $Q ^ { \pi _ { w } }$?</p>

<ul>
<li>Monte-Carlo policy gradient: it estimates the $Q ^ { \pi _ { w } } (s,a)$ from rollouts on the environment while following policy $\pi _ { w }$. It is unbiased and, without instabilities induced by bootstrapping. However, the estimate requires <strong>on-policy rollouts</strong> and can exhibit <strong>high variance</strong>. Several rollouts are typically needed to obtain a good estimate of the return (<strong>not sample efficient</strong>).</li>
<li>Actor-critic methods: use an estimate of the return given by a value-based approach, more efficient.</li>
</ul></li>

<li><p>Remarks:</p>

<ul>
<li>To prevent the policy from becoming deterministic, it is common to add an <strong>entropy regularizer</strong> to the gradient. With this regularizer, the learnt policy can remain stochastic. This ensures that the policy keeps exploring.<br /></li>
<li>Advantage value function: While $Q ^ { \pi _ { w } } (s,a)$ summarizes the performance of each action for a given state under policy $\pi _ w$, the advantage function $A ^ { \pi _ { w } } (s,a)$ provides a measure of comparison for each action to the expected return at the state $s$, given by $V ^ { \pi _ { w } } (s)$. Using $A ^ { \pi _ { w } } ( s , a ) = Q ^ { \pi _ { w } } ( s , a ) - V ^ { \pi _ { w } } ( s )$ has usually lower magnitudes than $Q ^ { \pi _ { w } } (s,a)$. This helps reduce the variance of the gradient estimator in the policy improvement step, while not modifying the expectation. The value function $V ^ {{ \pi } _ w} ( s )$ can be seen as a baseline or <strong>control variate</strong> for the gradient estimator. Using such a baseline allows for improved numerical efficiency – i.e. reaching a given performance with fewer updates – because <strong>the learning rate can be bigger</strong>.</li>
</ul></li>
</ul>

<p><font size=6>02/26: Deep Reinforcement Learning: Policy gradient methods (2)</font></p>

<p>Deterministic Policy Gradient</p>

<ul>
<li>Let us denote by $\pi (s)$ the deterministic policy: $\pi ( s ) : \mathcal { S } \rightarrow \mathcal { A }$. In discrete action spaces, a direct approach is to build the policy iteratively with:
$$\pi _ { k + 1 } ( s ) = \underset { a \in \mathcal { A } } { \operatorname { argmax } } Q ^ { \pi _ { k } } ( s , a )$$
where $\pi _ { k }$ is the policy at the $k$th iteration.</li>
<li>Deep Deterministic Policy Gradient (DDPG): In continuous action spaces, a greedy policy improvement becomes problematic, requiring a global maximization at every step. Instead, let us denote by $\pi _ w ( s )$ a differentiable deterministic policy. In that case, a simple and computationally attractive alternative is to move the policy in the direction of the gradient of $Q$:
$$\nabla _ { w } V ^ { \pi _ { w } } \left( s _ { 0 } \right) = \mathbb { E } _ { s \sim \rho ^ { \pi _ { w } } } \left[ \nabla _ { w } \left( \pi _ { w } \right) \nabla _ { a } \left. \left( Q ^ { \pi _ { w } } ( s , a ) \right) \right| _ { a = \pi _ { w } ( s ) } \right]$$
This equation implies relying on $\nabla _ { a } \left( Q ^ { \pi w } ( s , a ) \right)$ (in addition to $\nabla _ { a } \left( Q ^ { \pi w } ( s , a ) \right)$), which usually requires using actor-critic methods.</li>
</ul>

<p><font size=6>02/27: Deep Reinforcement Learning: Policy gradient methods (3)</font></p>

<p>Actor-Critic Methods</p>

<p>The actor refers to the policy and the critic to the estimate of a value function (e.g., the Q-value function). In deep RL, both the actor and the critic can be represented by non-linear neural network function approximators. The actor uses gradients derived from the policy gradient theorem and adjusts the policy parameters $w$. The critic, parameterized by $\theta$, estimates the approximate value function for the current policy $\pi$.
- The critic:
  - TD(0): at every iteration, the current value $Q(s,a;\theta)$ is updated towards a target value: $Y _ { k } ^ { Q } = r + \gamma Q \left( s ^ { \prime } , a = \pi \left( s ^ { \prime } \right) ; \theta \right)$. It is simple yet not computationally efficient as it uses a pure bootstrapping technique that is prone to instabilities and has a slow reward propagation backwards in time.
  - Retrace ($\lambda$): (i) it can make use of samples collected from any behavior policy without introducing a bias and (ii) it is efficient as it makes the best use of samples collected from near on-policy behavior policies. These architectures are sample-efficient thanks to the use of a replay memory, and computationally efficient since they use multi-step returns which improves the stability of learning and increases the speed of reward propagation backwards in time.
- The actor: the off-policy gradient in the policy improvement phase for the stochastic case is given as:
  $$\nabla _ { w } V ^ { \pi _ { w } } \left( s _ { 0 } \right) = \mathbb { E } _ { s \sim \rho ^ { \pi _ { \beta } } , a \sim \pi _ { \beta } } \left[ \nabla _ { \theta } \left( \log \pi _ { w } ( s , a ) \right) Q ^ { \pi _ { w } } ( s , a ) \right]$$
  where $\beta$ is a behavior policy generally different than $\pi$, which makes the gradient generally biased.
  - In the case of actor-critic methods, an approach to perform the policy gradient on-policy without experience replay has been investigated with the use of asynchronous methods, where multiple agents are executed in parallel and the actor-learners are trained asynchronously. The parallelization of agents also ensures that each agent experiences different parts of the environment at a given time step. In that case, n-step returns can be used without introducing a bias. It removes the need to maintain a replay buffer. However, it is not sample efficient.
  - An alternative is to combine off-policy and on-policy samples to trade-off both the sample efficiency of off-policy methods and the stability of on-policy gradient estimates. For example, Q-Prop uses a Monte Carlo on-policy gradient estimator, while reducing the variance of the gradient estimator by using an off-policy critic as a control variate. One limitation of Q-Prop is that it requires using on-policy samples for estimating the policy gradient.</p>

<p><font size=6>02/28: Deep Reinforcement Learning: Policy gradient methods (4)</font></p>

<p>Natural Policy Gradients</p>

<ul>
<li>Natural policy gradient methods use the steepest direction given by the Fisher information metric, i.e. the update follows the direction that maximizes $( J ( w ) - J ( w + \Delta w ) )$ under a constraint on $| \Delta w | _ { 2 }$.</li>
<li>In the hypothesis that the constraint on $\Delta w$ is defined with another metric than $L _ 2$, the first-order solution to the constrained optimization problem typically has the form $\Delta w \propto B ^ { - 1 } \nabla _ { w } J ( w )$ where B is an $n _ w \times n _ w$ matrix.</li>
<li>In natural gradients, the norm uses the Fisher information metric, given by a local quadratic approximation to the KL divergence $D _ {KL} \left( \pi ^ { w } | \pi ^ { w + \Delta w } \right)$. The natural gradient ascent for improving the policy π w is given by：
$$\Delta w \propto F _ { w } ^ { - 1 } \nabla _ { w } V ^ { \pi _ { w } } ( \cdot )$$
where $F _ { w }$ is the Fisher information matrix given by:
$$F _ { w } = \mathbb { E } _ { \pi _ { w } } \left[ \nabla _ { w } \log \pi _ { w } ( s , \cdot ) \left( \nabla _ { w } \log \pi _ { w } ( s , \cdot ) \right) ^ { T } \right]$$</li>
<li>As the angle between natural and ordinary gradient is never larger than ninety degrees, convergence is also guaranteed when using natural gradients. In the case of neural networks and their large number of parameters, it is usually impractical to compute, invert, and store the Fisher information matrix.</li>
</ul>

<p>Trust Region Optimization</p>

<ul>
<li>The policy optimization methods based on trust region restrict the changes in a policy using the KL divergence between the action distributions. By bounding the size of the policy update, trust region methods also bound the changes in state distributions guaranteeing improvements in policy.</li>
<li>Trust Region Policy Optimization (TRPO): uses constrained updates and advantage function estimation to perform the update, resulting in the reformulated optimization given by
$$\max _ { \Delta w } \mathbb { E } _ { s \sim \rho ^ { \pi } w , a \sim \pi } \left[ \frac { \pi _ { w + \Delta w } ( s , a ) } { \pi _ { w } ( s , a ) } A ^ { \pi _ w } ( s , a ) \right]$$
subject to $\mathbb { E }[ D _ { \mathrm { KL } } \left( \pi _ { w } ( s , \cdot ) | \pi _ { w + \Delta w } ( s , \cdot ) \right) ] \leq \delta$, where $\delta \in \mathbb{R}$ is a hyperparameter.</li>
<li>Proximal Policy Optimization (PPO):  it is a variant of the TRPO algorithm, which formulates the constraint as a penalty or a clipping objective, instead of using the KL constraint. PPO considers modifying the objective function to penalize changes to the policy that move r t ( w ) = π w+4w (s,a) π w (s,a) away from 1. The clipping objective that PPO maximizes is given by:
$$\underset { s \sim \rho ^ { \pi } w , a \sim \pi _ { w } } { \mathbb { E } } \left[ \min \left( r _ { t } ( w ) A ^ { \pi _ { w } } ( s , a ) , \operatorname { clip } \left( r _ { t } ( w ) , 1 - \epsilon , 1 + \epsilon \right) A ^ { \pi _ { w } } ( s , a ) \right) \right]$$
where $\epsilon \in \mathbb{R}$ is a hyperparameter. This objective function clips the probability ratio to constrain the changes of $r _ t$ in the interval $[1− \epsilon, 1+ \epsilon]$.</li>
</ul>

<h1 id="march">March</h1>

<p><font size=6>03/01: Deep Reinforcement Learning: Policy gradient methods (5)</font></p>

<p>Combining policy gradient and Q-learning</p>

<ul>
<li>Policy gradient algorithms have the following properties unlike the methods based on DQN:

<ul>
<li>They are able to <strong>work with continuous action spaces</strong>. This is particularly interesting in applications such as robotics, where forces and torques can take a continuum of values.</li>
<li>They can <strong>represent stochastic policies</strong>, which is useful for building policies that can explicitly explore. This is also useful in settings where the optimal policy is a stochastic policy (e.g., in a multi-agent setting where the Nash equilibrium is a stochastic policy).</li>
</ul></li>
<li>Combine policy gradient methods directly with off-policy Q-learning: In some specific settings, depending on the loss function and the entropy regularization used, value-based methods and policy-based methods are equivalent. For instance, when adding an entropy regularization, we have:
$$\nabla _ { w } V ^ { \pi _ { w } } \left( s _ { 0 } \right) = \mathbb { E } _ { s , a } \left[ \nabla _ { w } \left( \log \pi _ { w } ( s , a ) \right) Q ^ { \pi _ { w } } ( s , a ) \right] + \alpha \mathbb { E } _ { s } [ \nabla _ { w } H ^ { \pi _ { w } } ( s )]$$
where $H ^ { \pi } ( s ) = - \sum _ { a } \pi ( s , a ) \log \pi ( s , a )$. From this, one can note that an optimum is satisfied by the following policy: $\pi _ { w } ( s , a ) = \exp ( \frac{A ^ { \pi _ w } ( s , a )}{\alpha} - H ^ { \pi _ w } ( s ) )$. Therefore, we can use the policy to derive an estimate of the advantage function:
$$\tilde { A } ^ { \pi _ { w } } ( s , a ) = \alpha \left( \log \pi _ { w } ( s , a ) + H ^ { \pi } ( s ) \right)$$</li>
<li>Both value-based and policy-based methods are model-free and they do not make use of any model of the environment.</li>
</ul>

<hr />

<p><font size=6>03/02: Deep Reinforcement Learning: Model-based methods (1)</font></p>

<p>Pure model-based methods: When a model of the environment is available, planning consists in interacting with the model to recommend an action. In the case of discrete actions, lookahead search is usually done by generating potential trajectories. In the case of a continuous action space, trajectory optimization with a variety of controllers can be used.</p>

<ul>
<li>Lookahead search: limited to discrete actions

<ul>
<li>A lookahead search in an MDP iteratively builds a decision tree where the current state is the root node. It stores the obtained returns in the nodes and focuses attention on promising potential trajectories.</li>
<li>Monte-Carlo tree search (MCTS): The idea is to sample multiple trajectories from the current state until a terminal condition is reached (e.g., a given maximum depth). From those simulation steps, the MCTS algorithm then recommends an action to take.</li>
<li>Recent works have developed strategies to directly learn end-to-end the model, along with how to make the best use of it, without relying on explicit tree search techniques. These approaches show improved sample eﬃciency, performance, and robustness to model misspeciﬁcation compared to the separated approach (simply learning the model and then relying on it during planning). Why?</li>
</ul></li>
<li>Trajectory optimization:

<ul>
<li>If the model is differentiable, one can directly compute an analytic policy gradient by backpropagation of rewards along trajectories. For instance, PILCO uses Gaussian processes to learn a probabilistic model of the dynamics. It can then explicitly use the uncertainty for planning and policy evaluation in order to achieve a <strong>good sample efficiency</strong>. However, the gaussian processes have not been able to scale reliably to high-dimensional problems.</li>
<li>Wahlström et al. (2015) uses a deep learning model of the dynamics (with an auto-encoder) along with a model in a latent state space. Model-predictive control (Morari and Lee, 1999) can then be used to find the policy by repeatedly solving a finite-horizon optimal control problem in the latent space.</li>
<li>Watter et al. (2015) builds a probabilistic generative model in a latent space with the objective that it possesses a locally linear dynamics, which allows control to be performed more efficiently.</li>
<li>Another approach is to use the trajectory optimizer as a teacher rather than a demonstrator: <strong>guided policy search</strong> takes a few sequences of actions suggested by another controller. It then learns to adjust the policy from these sequences.</li>
</ul></li>
</ul>

<p><font size=6>03/03: Deep Reinforcement Learning: Model-based methods (2)</font></p>

<p>Integrating model-free and model-based methods: the respective strengths of the model-free versus model-based approaches depend on different factors.</p>

<ul>
<li>The best suited approach depends on whether the agent has access to a model of the environment. If that's not the case, the learned model usually has some inaccuracies that should be taken into account.</li>
<li>A model-based approach requires working in conjunction with a planning algorithm (or controller), which is often computationally demanding. The time constraints for computing the policy $\pi (s)$ via planning must therefore be taken into account (e.g., for applications with real-time decision-making or simply due to resource limitations).</li>
<li>For some tasks, the structure of the policy (or value function) is the easiest one to learn, but for other tasks, the model of the environment may be learned more efficiently due to the particular structure of the task (less complex or with more regularity). Thus, the most performant approach depends on the structure of the model, policy, and value function.
<br /></li>
</ul>

<p>How to obtain advantages from both worlds by integrating learning and planning:</p>

<ul>
<li>When the model is available, one direct approach is to use tree search techniques that make use of both value and policy networks.</li>
<li>When the model is not available and under the assumption that the agent has only access to a limited number of trajectories, the key property is to have an algorithm that generalizes well. One possibility is to build a model that is used to generate additional samples for a model-free reinforcement learning algorithm. Another possibility is to use a model-based approach along with a controller such as <strong>MPC</strong> to perform basic tasks and <strong>use model-free fine-tuning in order to achieve task success</strong>.</li>
<li>Other approaches build neural network architectures that combine both model-free and model-based elements. The VIN architecture (Tamar et al., 2016) is a fully differentiable neural network with a planning module that learns to plan from model-free objectives (given by a value function). It works well for tasks that involve planning-based reasoning (navigation tasks) from one initial position to one goal position and it demonstrates strong generalization in a few different domains.</li>
</ul>

<p>Improving the combination of model-free and model-based ideas is one key area of research for the future development of deep RL algorithms.</p>

<p><font size=6>03/04: Deep Reinforcement Learning: Generalization (1)</font></p>

<p>Generalization refers to either
- the capacity to achieve good performance in an environment where <strong>limited data</strong> has been gathered, or
- the capacity to obtain good performance <strong>in a related environment</strong>.</p>

<p>In the former case, the idea of generalization is directly related to the notion of sample efficiency (e.g., when the state-action space is too large to be fully visited). In the latter case, the test environment has common patterns with the training environment but can differ in the dynamics and the rewards. For instance, the underlying dynamics may be the same but a transformation on the observations may have happened.</p>

<p>Let us consider the case of a finite dataset $D$ obtained on the exact same task as the test environment. Formally, a dataset available to the agent $D \sim D$ can be defined as a set of four-tuples $(s, a, r, s^{\prime}) \in S \times A \times R \times S$ gathered by sampling independently and identically (i.i.d.).</p>

<ul>
<li>a given number of state-action pairs $(s,a)$ from some fixed distribution with $P(s, a)&gt;0$, $\forall (s,a) \in S \times A$;</li>
<li>a next state $s ^ {\prime} \sim P(s, a, \cdot)$;</li>
<li>a reward $r = R(s, a, s ^ {\prime})$;
We denote by $D _ {\infty}$ the particular case of a dataset $D$ where the number of tuples tends to infinity.</li>
</ul>

<p>A learning algorithm can be seen as a mapping of a dataset $D$ into a policy $\pi _ D$. Then we can decompose the suboptimality of the expected return as follows:</p>

<p>$$\underset { D \sim \mathcal { D } } { \mathbb { E } } [ V ^ { \pi ^ { * } } ( s ) - V ^ { \pi _ { D } } ( s ) ]$$
$$= \underset { D \sim \mathcal { D } } { \mathbb { E } } [ V ^ { \pi ^ { * } } ( s ) - V ^ { \pi _ { D \infty } } ( s ) + V ^ { \pi _ { D \infty } ( s ) } - V ^ { \pi _ { D } } ( s ) ]$$
$$ = \underbrace { ( V ^ { \pi ^ { * } } ( s ) - {V ^ { \pi _ { D _ { \infty } } } ( s ) ) } } _ { \text {asymptotic bias} } + \underbrace { \underset { D \sim \mathcal { D } } { \mathbb { E } } [ { V ^ { \pi _ { D \infty } } ( s ) } - V ^ { \pi _ { D } } ( s ) ] } _ { \text {error due to finite size of the dataset } D }
$$</p>

<p>This decomposition highlights two different terms: (i) an asymptotic bias which is independent of the quantity of data and (ii) an overfitting term directly related to the fact that the amount of data is limited.</p>

<p>Improving generalization can be seen as a tradeoff between (i) an error due to the fact that the algorithm trusts completely the frequentist assumption (i.e., discards any uncertainty on the limited data distribution) and (ii) an error due to the bias introduced to reduce the risk of overfitting. When the quality of the dataset is low, the learning algorithm should favor more robust policies (i.e., consider a smaller class of policies with stronger generalization capabilities). When the quality of the dataset increases, the risk of overﬁtting is lower and the learning algorithm can trust the data more, hence reducing the asymptotic bias.</p>

<p><font size=6>03/05: Deep Reinforcement Learning: Generalization (2)</font></p>

<p>We discuss the following key elements that are at stake when one wants to improve generalization in deep RL:</p>

<ul>
<li>the state representation;</li>
<li>the learning algorithm (type of function approximator, model-free vs model-based);</li>
<li>the objective function (e.g., reward shaping, tuning the training discount factor);</li>
<li>using hierarchical learning.</li>
</ul>

<p>Different aspects that can be used to avoid overfitting to limited data.</p>

<ul>
<li>Feature selection: The appropriate level of abstraction plays a key role in the bias-overﬁtting tradeoﬀ and one of the key advantages of using a small but rich abstract representation is to allow for improved generalization.

<ul>
<li>Overfitting: When considering many features on which to base the policy, an RL algorithm may take into consideration spurious correlations, which leads to overfitting.</li>
<li>Asymptotic bias: Removing features that discriminate states with a very different role in the dynamics introduces an asymptotic bias. The same policy would be enforced on undistinguishable states, hence leading to a sub-optimal policy.</li>
<li>In deep RL, one approach is to ﬁrst infer a factorized set of generative factors from the observations. This can be done for instance with an encoder-decoder architecture variant. These features can then be used as inputs to a reinforcement learning algorithm. The learned representation can, in some contexts, greatly help for generalization as it provides a more succinct representation that is less prone to overfitting. Some features may be kept in the abstract representation because they are important for the reconstruction of the observations, though they are otherwise irrelevant for the task at hand. Crucial information about the scene may also be discarded in the latent representation, particularly if that information takes up a small proportion of the observations $x$ in pixel space.</li>
</ul></li>
<li>Learning algorithm and function approximator selection:

<ul>
<li>If the function approximator used for the value function and/or the policy and/or the model is too simple, an asymptotic bias may appear. When the function approximator has poor generalization, there will be a large error due to the finite size of the dataset (overfitting).</li>
<li>One approach to mitigate non-informative features is to force the agent to acquire a set of symbolic rules adapted to the task and to reason on a more abstract level. This abstract level reasoning and the improved generalization have the potential to induce high-level cognitive functions such as transfer learning and analogical reasoning. For instance, the function approximator may embed a relational learning structure and thus build on the idea of relational reinforcement learning.</li>
<li>Auxiliary tasks: In the context of deep reinforcement learning, Jaderberg et al. (2016) show that augmenting a deep reinforcement learning agent with auxiliary tasks within a jointly learned representation can drastically improve sample efficiency in learning. This is done by maximizing simultaneously many pseudo-reward functions. The argument is that learning related tasks introduces an inductive bias that causes a model to build features in the neural network that are useful for the range of tasks. By explicitly learning both the model-free and model-based components through the state representation, along with an approximate entropy maximization penalty, the CRAR agent (François-Lavet et al., 2018) shows how it is possible to learn a low-dimensional representation of the task. In addition, this approach can directly make use of a combination of model-free and model-based, with planning happening in a smaller latent state space.</li>
</ul></li>
</ul>

<p><font size=6>03/06: Deep Reinforcement Learning: Generalization (3)</font></p>

<p>Modifying the objective function</p>

<p>In order to improve the policy learned by a deep RL algorithm, one can optimize an objective function that diverts from the actual objective. By doing so, a bias is usually introduced but this can in some cases help with generalization.</p>

<ul>
<li>Reward shaping: In practice, reward shaping uses prior knowledge by giving intermediate rewards for actions that lead to desired outcome. It is usually formalized as a function $F(s, a, s ^ {\prime})$ added to the original reward function $R(s, a, s ^ {\prime})$ of the original MDP. This technique is often used in deep reinforcement learning to improve the learning process in settings with sparse and delayed rewards.</li>
<li>Discount factor:

<ul>
<li>When the model available to the agent is estimated from data, the policy found using a shorter planning horizon can actually be better than a policy learned with the true horizon. On the one hand, artiﬁcially reducing the planning horizon leads to a bias since the objective function is modified. However, if a long planning horizon is targeted (the discount factor $\gamma$ is close to 1), there is a higher risk of overfitting. This can intuitively be understood as linked to the accumulation of the errors in the transitions and rewards estimated from data as compared to the actual transition and reward probabilities.</li>
<li>A high discount factor also requires specific care in value iteration algorithms as it can lead to instabilities in convergence. This effect is due to the mappings used in the value iteration algorithms with bootstrapping that propagate errors more strongly with a high discount factor. When bootstrapping is used in a deep RL value iteration algorithm, the risk of instabilities and overestimation of the value function is empirically stronger for a discount factor close to one.</li>
</ul></li>
</ul>
        </article>
  </div>
</section>

<aside id=comments>
    <div><h2> Comments </h2></div>
    <div id="disqus_thread"></div>
<script>
    var disqus_config = function () {
    
    
    
    };
    (function() {
        if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
            document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
            return;
        }
        var d = document, s = d.createElement('script'); s.async = true;
        s.src = '//' + "blog-lvohfvy22n" + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
</aside>

<footer>
  <div>
    <p>
    &copy; 2018 <span itemprop="author" itemscope itemtype="http://schema.org/Person">
        <span itemprop="name">Qingfeng Lan.</span></span>
        Powered by <a href="http://hugo.spf13.com">Hugo</a>.
        Theme by <a href="http://spf13.com">Steve Francia</a>.
    </p>
  </div>
</footer>
<script type="text/javascript">
(function(){var j=function(a,b){return window.getComputedStyle?getComputedStyle(a).getPropertyValue(b):a.currentStyle[b]};var k=function(a,b,c){if(a.addEventListener)a.addEventListener(b,c,false);else a.attachEvent('on'+b,c)};var l=function(a,b){for(key in b)if(b.hasOwnProperty(key))a[key]=b[key];return a};window.fitText=function(d,e,f){var g=l({'minFontSize':-1/0,'maxFontSize':1/0},f);var h=function(a){var b=e||1;var c=function(){a.style.fontSize=Math.max(Math.min(a.clientWidth/(b*10),parseFloat(g.maxFontSize)),parseFloat(g.minFontSize))+'px'};c();k(window,'resize',c)};if(d.length)for(var i=0;i<d.length;i++)h(d[i]);else h(d);return d}})();
fitText(document.getElementById('title'), 1)
</script>

<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/8.4/styles/default.min.css">
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/8.4/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad();</script>

</body>
</html>

</body>