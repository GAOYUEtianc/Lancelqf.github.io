<!DOCTYPE html>
<html class="no-js" lang="en-US" prefix="og: http://ogp.me/ns# fb: http://ogp.me/ns/fb#">
<head>
    <meta charset="utf-8">

    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
<meta name="description" content="">
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1">

<meta name="description" content="This is my research diary about intelligence in 2019.">
<meta name="keywords" content="Self, Research, ">

 
<meta property="og:type" content="article"/>
<meta property="og:description" content="This is my research diary about intelligence in 2019."/>
<meta property="og:title" content="Research Diary 2019 : lancelqf.github.io"/>
<meta property="og:site_name" content="Qingfeng Lan's blog"/>
<meta property="og:image" content="" />
<meta property="og:image:type" content="image/jpeg" />
<meta property="og:image:width" content="" />
<meta property="og:image:height" content="" />
<meta property="og:url" content="https://lancelqf.github.io/note/research_diary2019/">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2019-01-21"/>
<meta property="article:modified_time" content="2019-01-21"/>


<meta property="article:tag" content="Self">
<meta property="article:tag" content="Research">



    <base href="https://lancelqf.github.io">
    <title> Research Diary 2019 - lancelqf.github.io </title>
    <link rel="canonical" href="https://lancelqf.github.io/note/research_diary2019/">
    

    
<link rel="stylesheet" href="/static/css/style.css">
    <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon" />
    <link rel="apple-touch-icon" href="/apple-touch-icon.png" />
    
    <script type="text/javascript" async
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/MathJax.js?config=TeX-AMS-MML_HTMLorMML">

  MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$']],
    displayMath: [['$$','$$']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
  });

  MathJax.Hub.Queue(function() {
    
    
    
    var all = MathJax.Hub.getAllJax(), i;
    for(i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<style>
code.has-jax {
  font: inherit;
  font-size: 100%;
  background: inherit;
  border: inherit;
  color: #515151;
}
</style>

    <script>
      var _hmt = _hmt || [];
      (function() {
        var hm = document.createElement("script");
        hm.src = "https://hm.baidu.com/hm.js?2445edbea2f2541f3150c49cc08c381a";
        var s = document.getElementsByTagName("script")[0]; 
        s.parentNode.insertBefore(hm, s);
      })();
    </script>      

    
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-121043808-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-121043808-1');
    </script>
    
</head>
<body lang="en" itemscope itemtype="http://schema.org/Article">
<header id="header">
  <nav id="nav">
    <div id="title"><a href="/" class="blue"><font color="blue">Qingfeng Lan</font></a></div>
    <div><a href=mailto:lancelqf@gmail.com target="_blank" class="blue"><span class="icon-mail"></span></a></div>
  </nav>
  <nav id="nav">
    <ul id="mainnav">


  <li>
    <a href="/heart/">
    <span class="icon"> <i aria-hidden="true" class="icon-heart"></i></span>
    <span> Heart </span>
  </a>
  </li>

  <li>
    <a href="/note/">
        <span class="icon"> <i aria-hidden="true" class="icon-pencil"></i></span>
        <span> Note </span>
    </a>
  </li>

  <li>
  <a href="/tech/">
      <span class="icon"> <i aria-hidden="true" class="icon-gears"></i></span>
      <span> Tech </span>
  </a>
  </li>

  <li>
  <a href="/about">
      <span class="icon"> <i aria-hidden="true" class="icon-atom"></i></span>
      <span> About </span>
  </a>
  </li>

</ul>
    <ul id="social">
  <li id="share">
      <span class="title"> share </span>
      <div class="dropdown share">
          <ul class="social">
              <li> <a href="http://spf13.com/" target="_blank" title="spf13 is Steve Francis" class="facebook">spf13</a> </li>
              <li> <a href="http://nanshu.wang/" target="_blank" title="Dear Sister Nanshu" class="nanshu">Nanshu</a> </li>
              <li> <a href="http://spaces.facsci.ualberta.ca/rlai/" target="_blank" title="A RL&AI researh group at the University of Alberta" class="twitter">RLAI</a> </li>
              <li> <a href="https://deepmind.com/" target="_blank" title="An AI research company, part of the Alphabet group" class="googleplus">DeepMind</a> </li>
              <li> <a href="https://openai.com/" target="_blank" title="A non-profit AI research company" class="stumbleupon">OpenAI</a> </li>
              <li> <a href="http://whirl.cs.ox.ac.uk/" target="_blank" title="A ML research group at the University of Oxford" class="delicious">WhiRL</a> </li>             
          </ul>
          <span class="icon icon-bubbles"> </span> <span class="subcount"> </span>
      </div>
  </li>

  <li id="follow">
      <span class="title"> follow </span>
      <div class="dropdown follow">
          <ul class="social">
              <li> <a href="https://www.linkedin.com/in/qingfeng-lan-974685146" target="_blank" title="LinkedIn" class="linkedin">LinkedIn</a> </li>
              <li> <a href="https://github.com/qlan3" target="_blank" title="GitHub" class="github">GitHub</a> </li>
              <li> <a href="https://www.instagram.com/qingfeng_lan/" target="_blank" title="Instagram" class="facebook">Instagram</a> </li>
              <li> <a href="https://twitter.com/lancelan3" target="_blank" title="Twitter" class="twitter">Twitter</a> </li>
          </ul>
          <span class="icon icon-rocket"> </span> <span class="subcount"></span>
      </div>
  </li>
</ul>
  </nav>
</header>


<section id="main">
  <h1 itemprop="name" >Research Diary 2019</h1>
  

<aside id="meta">

    <div>
        <section id="datecount">
          <h4 id="date"> Mon Jan 21, 2019 </h4>
        </section>
        <ul id="tags">
          
            <li> <a href="https://lancelqf.github.io/tags/self">Self</a> </li>
          
            <li> <a href="https://lancelqf.github.io/tags/research">Research</a> </li>
          
        </ul>
    </div>
</aside>

<meta itemprop="wordCount" content="5191">
<meta itemprop="datePublished" content="2019-01-21">
<meta itemprop="url" content="https://lancelqf.github.io/note/research_diary2019/">

  <div>
        <article itemprop="articleBody" id="content">
           <p>This is my research diary about intelligence in 2019. I find writing research diary really interesting and it helps me to organize my thougths and discover new ideas! I just hope I have enough thougths to continue writing.</p>

<p></p>

<h1 id="reference">Reference</h1>

<ul>
<li><a href="http://incompleteideas.net/book/the-book.html">Reinforcement Learning: An Introduction. Richard S. Sutton and Andrew G. Barto, Second Edition, MIT Press, Cambridge, MA, 2018.</a></li>
<li><a href="http://aima.cs.berkeley.edu/">Artificial Intelligence: A Modern Approach. Stuart Russell and Peter Norvig, Third Edition, Prentice Hall, 2009.</a></li>
</ul>

<h1 id="january">January</h1>

<p><font size=6>01/21: What is intelligence? (1/2)</font></p>

<p>I'm going to talk about intelligence. To be specific, it is not just human intelligence or artificial intelligence but general intelligence that I want to talk about. Although it is really hard to find a proper and satisfying defnition for intelligence, it is still possible to name some traits of intelligence. Let's begin!</p>

<ul>
<li><p><strong>Adaptive</strong>: Intelligence is adaptive and flexible, easy to adjust itself. It should have a strong adaptivity that enables it to adapt to any environments. Image that a baby born on Mars or Moon. Although the gravaity is different than it on Earth, it won't take the baby too long to get used to the new environment. For human being or other forms of life, this ability is essential to their survival. So it is for an intelligent agent. Afterall, an agent that fails to survive cannot have enought time to do what it needs to do.<br />
But what does this abstract word, adaptive, really mean? Well, it is about making right decisions. But what is right? And how to make right descisions? Since I don't have a clear answer rigt now, it is better to leave it for future elaboration.</p></li>

<li><p><strong>Robust</strong>: An intelligent agent should be complex enough to cope with possible errors. There are basic two ways: the self-correcting way and the fault-tolerant way. The self-correcting way is more active while the fault-tolerant way is more passive. Robustness overlaps with self-optimizing (see below).</p></li>

<li><p><strong>Self-optimizing</strong>: While making actions is about changing the outside state, self-optimizing is about changing the inner state of the agent itself. Self-improving is a simliar word. Evolving has a broader meaning. It emphasizes changing, not just improving or optimizing. Updating knowledge system and optimizing action outputs are two good examples. In a word, the agent improves itself in order to better achieve the goal.</p></li>
</ul>

<p>Note that the three traits mentioned above are strongly connect with each other.</p>

<p><font size=6>01/22: What is intelligence? (2/2)</font></p>

<ul>
<li><p><strong>Efficient</strong>: In real world, we often need to solve optimzation problems, such as finding the shortest way to school, makeing as much as money given fixed time and being sucessful as young as possible. In short, we want to achieve the goal efficiently. Similarly, a good intelligent agent should be efficient enough to realize the final goal. For example, energy saving and less time wasting. However, this may not always be the case because I am talking about efficiency in the sense of archieving goal. The goal can also be related to inefficiency, such as riding a bike as slow as you can. So the efficiency I mean, is based on the goal totally. Planning and optimization play important roles here.</p></li>

<li><p><strong>Learning ability</strong>: Although by hardcoding knowledge into an agent's &quot;brain&quot; may solve most practical problems, it requres a great amount of manual work. Moreover, it is not flexiable——hardcoding means it is not easy to update current knowledge thus making self-optimizing impossible.<br />
Instead of trying to produce a programme to simulate the adult mind, why not rather try to produce one which simulates the child's? (Alan Turing) Children have incredible learning ability and they learn fast. They are born with the ability to learn which is encoded in DNA during the long period of evolution.<br />
Life long learning, few shot learning, multi-task learning, multi-agent Learning, meta learning and transfer learning can all be included in this part. Next, I will introduce two aspects of learning:</p>

<ul>
<li><strong>learning knowledge</strong>: This is the most obvious aspect of learning. It is a process of extracting features, patterns and rules from raw data generated by the interaction between the agent and the environment (direct experience) or from existing data (indirect experience). It's similar to discovery of physics laws. Supervise and unsupervise learning are two good examples. Moreover, after discovering new knowledge, it is also necessary to embed it into current knowledge system and to make connections from old knowledge to new knowledge. By doing this, an agent can accumulate knowledge as time goes on and wield knowledge in a more powerful way. Finally, the agent is able to construct a model of the world.<br />
Is learning knowledge an application of funtion approximation? Can any knowledge be expressed as functions? To answer these questions, we need to define knowledge itself. Generally, what we call knowledge can be divided into two subgroups: the knowledge to environment and meta knowledge (the knowledge of knowledge itself). I leave it for future consideration.</li>
<li><strong>learning how to learn</strong> (also known as <strong>meta learning</strong>): Learning is not only about learning knowledge but also learning how to learn. I call this kind of knowledge as meta knowledge (the knowledge of knowledge itself). It is more abstract than the knowledge of environment, similar to (but not the same as) the relation between physics and mathematics (This is still not a good comparsion, but the best I can find right now.). Things like how to discover knowledge more quickly and when/where/how to apply it are two examples.</li>
</ul></li>

<li><p><strong>Others</strong>: creativity, imagination... The meaning of these words are vague and abstract. I don't know how to translate them into more concrete ones. Maybe they are just manifestations of searching, exploring and combining old things into new things.</p></li>
</ul>

<p><font size=6>01/23: What's inside in an intelligent agent?</font></p>

<p>The traits of intelligence are like softwares in a computer. So what hardwares do we need in order to support the normal running of softwares? In my opinion, these subsystems are necessary for an intelligent agent:</p>

<ul>
<li><strong>Sensor</strong>: Any things that preceives the environment counts as a sensor. An agent may have many sensors. A sensor can be defined as a mapping from environment to signals. For example, an ear transforms sound waves to acoustical signals and an eye transforms photons of lights to images. The sensors create what the agent can &quot;see&quot;. The signals that sensors output are the only information source of the environment. Anything that cannnot be perceived by sensors does not exist with regard to the agent. Note that what sensors perceive may be wrong, i.e. the signals produced by sensors do not reflect the real world accurately.</li>
<li><strong>Actor</strong>: An actor is anything that an agent can control to influence the outside environment. It receives commands from the decision make and then act according to the commands appropriately. The actions  include simple reflex action, model-based action and goal-based action. They can be atomic, factored or structured.</li>
<li><strong>Decision maker</strong>: It is the brain of an agent and also the most important subsystem for decision making. The inputs are the signals received by sensors. And it outputs commands which are excuted by actors. To analyze it meticulously, I divide it into several parts.

<ul>
<li><strong>Knowledge base</strong>: A knowledge base is where an agent stores knowledge.</li>
<li><strong>Thinker</strong>: It is the core of descision maker and also employs most computation resources. It makes a series of decisions given access to knowledge base and signals from sensors. If necessary, it also updates knowledge in the knowledge base or even create a new one from scatch. It is the basis of intelligence. All other parts only exist to support it. I don't have too many ideas about it and may spend the rest of my life to figure out how to make one.</li>
</ul></li>
</ul>

<p>Can we separate the knowledge base with the thinker totally? Yes! One good example is McCarthy's Advice Taker in which the rules and the reasoning component are separated. But is it good? I'm not sure. It's possible that these subsystems are strongly connected or even merged together without clear boundaries. If the knowledge base and the planner are separated completely, it may take more time to access necessary knowledge before making a proper decision. Thus, a distributed knowledge base may be more efficient. For example, knowledge in neural networks is represented by weight matrice between layers. The decision making process consists of repetitions of acessing to knowledge (weights) and computation (compute activation functions, do matrix multiplication, etc.). The two procedures are followed one after the other in the forward process. Further thoughts and emprical experiments are still needed.</p>

<p><font size=6>01/24: Knowledge base</font></p>

<p>Before considering the knowledge base, it is better starting from more basic questions.</p>

<ul>
<li><p>What avaliable knowledge representations we can use?</p>

<ul>
<li><strong>Human language (written language)</strong>: Almost any knowledge can be expressed as language. It has inner structures, mapping from symbols to real world objects or even from symbols to symbols, full of bootstraping and self-reference. It evolves as time goes.<br /></li>
<li><strong>Vector</strong>: Methods such as word embedding, map words or phrases to vectors of real numbers. Since we can encode human language into a sequence of bytes (i.e. numbers) under some standard rules, any knowledge represented by human language can also be represented by a bunch of numbers (i.e. vectors). Afterall, they are all symbols.</li>
<li><strong>Graph</strong>: Knowledge graphs are human understandable and easy for computers to operate. They are also very powerful to represent knowledge. Maybe it can be a good interlanguage between human and computers. I don't know too much about this. <strong>Need future study</strong>.</li>
</ul></li>

<li><p>What is a good knowledge representation?<br />
In fact, whether a knowledge representation is good or bad really depends on the agent itself and the goal. I should say that any knowledge that helps the agent to achieve the goal is a good one. Although it is hard to give a clear answer, typically, a good knowledge representation is:</p>

<ul>
<li><strong>powerful</strong>: It should be able to express enough knowledge.</li>
<li><strong>compact</strong>: The representation should have high information density which is space-saving.</li>
<li><strong>easy to operate</strong>: It should be easy to handle and operate by the agent.<br /></li>
</ul>

<p>Note that a good representation is not necessary human readable although it will be really helpful.</p></li>

<li><p>Now let's consider knowledge base. A good knowledge base is:</p>

<ul>
<li><strong>easy to retrive</strong>: It should be easy (less time consuming) for agent to find necessary knowledge.</li>
<li><strong>easy to update</strong>: It should be easy to add, delete and change knowledge in the database.</li>
<li><strong>space-saving</strong>: If the knowledge is too small, then there will not be enough space to store all knowledge the agent discovers. However, if it is too big, it takes a large amount of time to access. Both cases are not favourable. The best way to resolve this conflict is to find a space-saving way to store knowledge. Some compression methods can be applied if it is necessary, suitable and appropriate.</li>
</ul></li>

<li><p>How do we (i.e. human being) store and retrieve memory? No idea. <strong>Need future study</strong>.</p></li>
</ul>

<p><font size=6>01/25: Reward function (1/3)</font></p>

<p>How to guide an agent to achieve the final goal. Use reward! (But do we really need it? Is it necessary?)</p>

<ul>
<li>Are binary ((0,1),(-1, +1)) or integer reward functions as powerful as any real number reward functions?<br />
&quot;Powerful&quot; is not very clear. What I mean is that can any policy generated using real number reward functions be generated by binary reward functions? I think the answer should be &quot;Yes&quot;. In general value iteration, we first do policy evaluation by computing state values or state-action values based on reward sequence we receive. After that, policy improvement is applied to find better policies. To be more specific, we need the information about how to choose the best action given state values or state-action values. How do we do this? We compare numbers and then choose the action with max state-action value. In fact, it doesn't matter whether these numbers are real numbers or integers. What matters is the order. And integer is an ordered field which should be enough. However, real reward function has a big advantage. Since real number field is dense, we can always find a real number between two different real numbers. For example, asumme that we assign $1$ to $Q(s,a _ 1)$ and $2$ to $Q(s,a _ 2)$. Then here comes $Q(s,a _ 3)$. And we find that $a _ 3$ is better than $a _ 1$ but worse than $a _ 2$, i.e. $1=Q(s,a _ 1) &lt; Q(s,a _ 3) &lt; Q(s,a _ 2)=2$. We can always find a real number between 1 and 2 for $Q(s,a _ 3)$ but not an integer. If we insist to use integers, we must adjust the value of $Q(s,a _ 1)$ and $Q(s,a _ 2)$ as well. So apparently, real number reward functions are easier to use. But binary and integer reward functions have their own merits——they are easier to design. Which one is more sample efficient? Which one helps the agent learn faster? <strong>Need future study</strong>.</li>
</ul>

<hr />

<p><font size=6>01/26: Reward function (2/3)</font></p>

<p>What are we talking about when we talk about rewards?</p>

<p>What are rewards? Where do they come from? Look around and then you will find that there are many things that we call reward, such as money paid by your boss, delicious food, compliment from people who respect you. But are they really reward? Definitely not! They are just what they are. We create rewards ourselves. Rewards come from our brain, especially the chemical reward system in the brain. Through the chemical reward system, the brain guides the body, drive it for food and away from danger. In short, rewards are interpretations of current state (both the environment and agent itself). They are a measure how good or better the current state is. And how close are we from the final goal. Typically, we use a real number to indicate this. The larger the reward value is, the better the current state is and the closer we are from the final goal. For example, if the goal is finding the shortest path from $cityA$ to $cityB$, the reward can be the inverse of path length from current position to $cityB$.</p>

<p>Note that the reward I'm talking about here is not same as the reward descriped in Reinforcement Learning. The reward in RL defines the goal, indicates what is good or bad in an immediate sense. In this context, however, it indicates what is good or bad in the long run. So for consistency, I call the reward that I mentioned before <strong>cumulation</strong>, denoted as $C_t$ at time $t$. And the new reward $R_t$ is defined as a function of cumulation, i.e. $R _ t \doteq C _ t - C _ {t-1}$. In another view, the cumulation is a cumulative sum of the reward sequence: $C _ t = R _ 1 + R _ 2 + \cdots + R _ t = \sum _ { k = 1 } ^ { t } R _ { k }$. The new reward is same as the reward in RL. But the cumulaiton is not return. As a reminder, the return is defined as $G _ { t } \doteq R _ { t + 1 } + R _ { t + 2 } + R _ { t + 3 } + \cdots + R _ { T }$. The return is a accumulation of future rewards while cumulation is a accumulation of past rewards. They are two aspects.</p>

<p>Similar to discounted return, we can also add discount factor to cumulation: $C _ t = R _ 1 + \gamma R _ 2 + \cdots + {\gamma} ^ {t-1} R _ t = \sum _ { k = 1 } ^ { t } \gamma ^ { k - 1 } R _ { k }$ and $R _ t = \frac{C _ t - C _ {t-1}}{{\gamma} ^ {t-1}}$.</p>

<p><font size=6>01/28: Reward function (3/3)</font></p>

<p>As I elaborated in last research diary, &quot;Rewards are interpretations of current state (both the environment and agent itself). They are a measure how good or better the current state is. And how close are we from the final goal.&quot; So $R _ t$ should be a function of environment state $s _ {env}$ and agent state $s _ {agent}$, i.e. $R _ t: S _ E \times S _ A \rightarrow \mathbb{R}$ where $S _ E$ is the environment state space and $S _ A$ is agent state space. In current typical RL setting, $R _ t$ is only determined by the environment. The agent state is not considered. This is because first, the reward design remains to be a difficult problem. Second, most RL agents are too simple. There are no such thing as agent states!</p>

<p>Instead of designning reward functions manually, can we design some methods to learn reward functions? For example, can we apply neural networks to approximate the true rewards (if they do exist) through bootstrapping? I think there should be some previous works (It's so obvous afterall!), or maybe someone tries this idea before but it didn't work. <strong>Need future study</strong>.</p>

<p><center><img src="/media/agent–environment_interaction.jpg" alt="The agent–environment interaction in a MDP." height="70%" width = "70%" /></center></p>

<p><font size=6>01/29: State</font></p>

<p>Define the state space $S$ to be $S \doteq S _ E \times S _ A$ where $S _ E$ is the environment state space and $S _ A$ is the agent state space. For every state $s$, it can written as $s=(s _ e, s _ a)$. If we use a vector to represent a state $s$, then each dimension of this vector is a feature of this state (e.g. $s=(f _ 1, f _ 2, \cdots, f _ n)$).</p>

<p>One advantage of representing states using vectors is that vectors are simple, comprehensible and easy to handle—they are just a bunch of features. However, since there is no internal structure inside a vector, it is not convenient to represent complex states or encode structural information in it, such as feature interaction (e.g. ${f _ 1 ^ 2} f _ 2$). One way to fix this is adding new features (e.g. $f _ {n+1} = {f _ 1 ^ 2} f _ 2$). But this usually leads to the curse of dimensionality. Thus we need a more compact but still powerful enough representation. And they should be manageable by agents (e.g. computers).</p>

<p>How about structural representations, such as trees and graphs? Are they good representations for states? Well, I'm not sure. Take the graph for example. If nodes are objects or features, edges that connect nodes can be the relations between objects or features. They are more powerful than vectors, since ordered disconnected nodes are equivalent to vectors. Moreover, nodes may have inner structure. In fact, we can even include part of graph inside a node! However, since graphs are more complex than vectors, they are also more difficult to handle with. And what is the edge that connects two nodes in the sense of mathematics? <strong>Need future study</strong>.</p>

<p><font size=6>01/30: Action (1/2)</font></p>

<p>Let $A$ be the action space. Then an action can be viewed as a mappings from a distribution of state space to another distribution of state space, i.e. $A: p(S) \rightarrow p(S)$. Note that this includes the mappings from one state to another state if all probability is concentrate on a particular state, i.e. $p(s)=1$ for state $s$ while $p(\cdot)=0$ for other states. For example, $a(p _ 1)=p _ 2$ where $p _ 1(s)=\mathcal{N}(0,1)$ and $p _ 2(s)=\mathcal{N}(1,4)$.</p>

<p>If an action only influences the variation between two distributions, we have following properties:</p>

<ul>
<li>$\Delta {p _ a} \doteq p _ 1 - p _ 2$ is only determined by action $a$ where $a(p _ 1)=p _ 2$. To be specific, $p _ 2 \doteq a(p _ 1)=p _ 1 + \Delta {p _ a}$.<br /></li>
<li>$\int_{s \in S} \Delta {p _ a} ds = 0$</li>
</ul>

<p>Note that this is still not well-defined. For example, $\Delta {p _ a}(s) = s$ for $s \in S=[-1,1]$. $p _ 1$ is a uniform distribution, defined as $p _ 1(s) = 1/2$. Then $p _ 2 \doteq a(p _ 1) = p _ 1 + \Delta {p _ a} = s + 1/2$. Easy to see $p _ 2 (s=-1) = -1/2$. However, $p _ 2$ is a probability density function, so $p _ 2 (s) \geq 0$ for all $s \in S$.</p>

<p>A possible way to fix this: we move the distribution function by a constant $c$, i.e. $p(s) =p(s) + c$ such that $\int_{s \in S} {max(p(s)+c, 0)} ds = 1$. The $p _ 2$ in the above example then becomes $p _ 2(s) = s + \sqrt{2} - 1$ for $s \in  [1 - \sqrt{2}, 1]$ . This is not a good solution since the computation of $c$ is not easy.</p>

<p>However, if we use sampling, a negative probability won't be a big problem. We can always sample from states that have a positive probability and discard the state we sampled out whose probability is negative.</p>

<h1 id="february">February</h1>

<p><font size=6>02/01: Action (2/2)</font></p>

<p>Recall that $p(s ^ { \prime } | s , a)$ is the probability of transition from state $s$ to $s ^ {\prime}$ under action $a$. We also have:
$$p _ 2(s ^ {\prime})= \sum_{s \in S} p(s ^ { \prime } | s , a) p _ 1(s) \quad \textrm{where} \quad a(p _ 1) = p _ 2$$</p>

<p>However, we can not recover state transition probability $p(s ^ { \prime } | s , a)$ simply from $p _ 1(s)$ and $p _ 2(s)$ since there are more unknown values than equations. This means that we can not have all information from $a(\cdot)$. But is $a(\cdot)$ useful enough? I don't know actually...</p>

<p>We divide all actions based on state space $S$ roughly in three categories:</p>

<ul>
<li>Atomic actions: They have no internal structures and are indivisible. They are the lowest level actions an agent can act. They are simple but also important. They are the points in the action space $A$. For completeness, we also add the identity action $a _ I$ in $A$ where $a _ I(p)=p$ for all $p$.</li>
<li>Composite actions: They are the actions in $A ^ n$ where $n \in \mathbb{N}$. For each composite action $a$, it is defined as $a \doteq a _ 1 \circ \dots \circ a _ n$ where $a _ i \in A$ for all $i \in {1, 2, \dots ,n}$.</li>
<li>Total actions: They are all possible actions that th agent can use, denote as $A ^ \infty$.  For each action $a \in {A ^ \infty}$, it is defined as $a \doteq a _ 1 \circ a _ 2 \circ \dots \circ \dots = \circ _ {i=1} ^ {\infty} a _ i$ where $a _ i \in A$ for all $i \in \mathbb{N}$.</li>
</ul>

<hr />

<p><font size=6>02/02: Hierarchical states and actions</font></p>

<p>Hierarchical states and actions are quite useful in planning and solving other problems. One way of constructing hierarchical states is abstraction (any other ways?). Then the natural questions are: what is abstraction and how to do that?</p>

<p>Well, abstraction is about leaving out unnecessary details, focusing on only general characteristics. Instances that have the same abstraction should share something in common. Based on this idea, we define a <strong>abstract state set</strong>:
 $$AS_f \doteq [x \in \Omega | f(x) \in P ]$$
 (There is a problem with the display of {}, I use [] instead.)
 where $\Omega$ is a set of instances; $f$ is a transform function that the abstract state set bonds to; $P$ is a set that represents some properties. For example, $\Omega = \mathbb{R} ^ n$; $f: \mathbb{R} ^ n \rightarrow \mathbb{R} ^ m$ is a projection function: $f(x) = y$ where $x=(v _ 1,\cdots,v _ n)$ and $x=(v _ 1,\cdots,v _ m)$, $m&lt;n$; $P=[x=(v _ 1,\cdots,v _ m) \in \mathbb{R} ^ m | v _ 1 = \cdots = v _ m ]$. Under this setting, $AS_f = [x=(v _ 1,\cdots,v _ n) \in \mathbb{R} ^ n | v _ 1 = \cdots = v _ m ]$.</p>

<p>Tile coding and state aggregation are two ways of defining abstract state set. Perhaps a more specific and simple way of defining abstract state set is clustering. The idea behind is very simple: instances that near each other under some metric should belong to a same abstract class. This can be viewed as doing clustering in the instance space.</p>

<p>Moreover, we can continue to define the second level abstract state set based on the (first) abstract state set:
  $$AS _ {f} ^ {2} \doteq [x \in \Omega | f(x) \in P ]$$
  where $\Omega$ is a subset of $AS _ {g}$. Similarly, we can define $n$-th level abstract state set $AS _ {f} ^ {n}$.</p>

<p>Similar to how we define the action on state space, we can define the $n$-th level action on the $n$-th abstract state set.</p>

<p><font size=6>02/04: Model (1/2): What is a model?</font></p>

<p>What is a model? According to Collins dictionary, &quot;A model of an object is a physical representation that shows what it looks like or how it works. The model is often smaller than the object it represents.&quot; In this definition, there are two important aspects:</p>

<ol>
<li>The model is often smaller than the object it represents.</li>
<li>The model help us understand how the object looks like or how it works. Furthermore, the model helps us predict the behavior of the object.</li>
</ol>

<p>What mathematical tools should we use to represent a model? Well, right now, for discrete process, we have deterministic finite automaton (DFA), nondeterministic finite automaton (NFA) and Markov decision process (MDP). In fact, they are quite similar. For continuous case, I don't know too much yet. <strong>Need future study</strong>. So for the rest, I focus only on discrete case.</p>

<p>Based on MDP, I define a simliar but also different mathematical framework, called Markov transition process (MTP):</p>

<blockquote>
<p>A MTP is a 5-tuple ($S, A, P _ a, S _ 0, F$), where<br />
  1. $S$ is a finite set of states.<br />
  2. $A$ is a finite set of actions.<br />
  3. $P _ {a}$ is the state transition probability defined as $a(p _ 1)=p _ 2$ for each action $a \in A$. If $p _ 1 (s) = 1$ for an action $a$, then we have $p _ 2 (s ^ {\prime}) = P _ {a} (s _ {t+1}=s ^ {\prime} \mid s _ {t}=s) = \operatorname{Pr}(s _ {t+1}=s ^ {\prime} \mid s _ {t}=s, a _ t = a)$ which is the probability that action $a$ in state $s$ at time $t$ will lead to state $s ^ {\prime}$ at time $t+1$.<br />
  4. $S _ 0 \subset S$ is a set of start states.<br />
  5. $F \subset S$ is a set of goal states.</p>
</blockquote>

<p>Note that the hierarchical states and actions also fits the definition of MTP simply by replacing $Q$ and $A _ s$ whith $n$-th level abstract state set and $n$-th level action set, respectively.</p>

<p>Also, sometimes we need to model a world without the interference of the agent. We can include this case by adding null action denoted as $\emptyset$. Thus $P _ {\emptyset}$ the state transition probability for null action.</p>

<p><font size=6>02/05: Model (2/2): Rethink</font></p>

<p>Things I need to consider further:</p>

<ol>
<li>The above model is still too simple. Afterall, the world is not Markov.</li>
<li>The world is changing and evolving eternally. How can the above model deal with a changing world? How to update?</li>
<li>The world is so complex and there are so many possible states and actions. Thus it is impossible to store all state transition probability. Possible solutions：

<ol>
<li>Use function approximation.</li>
<li>Only save the most useful/important/relevent/latest state transition probability. Introduce forgetting mechanism.</li>
<li>Instead of saving a probability distribution, we save several next states with large probability.</li>
</ol></li>
<li>Sometimes we don't know the true state transition probability but only transition samples. Thus we need to calculate the estimated state transtion probability from samples. However, since all estimation is inaccurate and induce variance. How to resolve this problem without getting more samples?</li>
<li>How can we apply transfer learning or one-shot learning to new states transitions?</li>
<li>How do I predict the world? Do I have a world model in my brain? Do I store a set of states somewhere in my brain? If so, how does the brain represent a state?</li>
<li>How much information can we store in our brain in all our lives?</li>
</ol>

<p><font size=6>02/07: Policy (1/2)</font></p>

<p>According the definition in Sutton's book, a policy defines the learning agent's way of behaving at a given time; a mapping from perceived states of the environment to actions to be taken when in those states.</p>

<p>In RL setting, typically there are two ways to learn a policy:<br />
1. Value-based methods: These methods (such as state value methods and state-action value methods) learn the action values and then select actions based on the estimated action values.<br />
2. Policy gradient methods: Instead of estimating action values, these methods learn a parameterized policy that select actions without a value function. A value function may still be used to learn the policy parameter, but is not required for action selection (Sutton, 2018).</p>

<p>My question is that are there any other approaches to generate a policy? For example, rule-based methods? Futhermore, do we have to use a reward fucntion? Is it really necessary?</p>

<p>Instead, can we learn a metric $M$ that measures how &quot;close&quot; is the current state to the goal states? If so, then what supervised information can we use to correct a wrong measurement? But is this method really different with value-based methods? Maybe not.</p>

<p><font size=6>02/08: Policy (2/2)</font></p>

<p>Anyway, let's try! Define the metric over (abstract) state set: $M: S \times S \rightarrow [0,+\infty)$. Since it is a metric, for $x,y,z \in S$, the following conditions are satisfied:</p>

<blockquote>
<ol>
<li>non-negativity: $M(x,y) \geq 0$</li>
<li>identity of indiscernibles: $M(x, y)=0 \Leftrightarrow x=y$</li>
<li>symmetry: $M(x,y)=M(y,x)$</li>
<li>triangle inequality: $M(x,z) \leq M(x,y) + M(y,z)$</li>
</ol>
</blockquote>

<p>We also have $M(F, F)=0$. Our goal is to reach a state $s$ such that $s= \arg \min _ {s \in S} M(s,F)$. Denote $\operatorname{Pr}(s _ {t+1}=s ^ {\prime} \mid s _ {t}=s, a _ t = a)$ as $\operatorname{Pr}(s ^ {\prime} \mid s, a)$ for short. Then for greedy action selection:
$$\pi (s)=\arg \min _ {a \in A} \Sigma _ {s ^ {\prime} \in S} \operatorname{Pr}(s ^ {\prime} \mid s, a) M(s ^ {\prime}, F)$$
or more generally:
$$\pi (s)=\arg \min _ {a \in A} \Sigma _ {s ^ {\prime} \in S} p _ 2(s ^ {\prime}) M(s ^ {\prime}, F)  \text{ where } a(p _ 1)=p _ 2$$
Compared with value-based policy,
$$
\pi (s)=\arg \max _ {a \in A} Q(s, a) = \arg \max _ {a \in A} \Sigma _ {s ^ {\prime} \in S} \operatorname{Pr}(s ^ {\prime} \mid s, a) (R + \gamma V(s ^ {\prime}))
$$
notice that if $M(s ^ {\prime}, F) \propto -(R + \gamma V(s ^ {\prime}))$, the two ways of generating policy are exact same. So it seems that we still need use something similar to reward or state value as a measurement of the agent's performance... Maybe this can be combined with some heuristic search algorithm or used as a better initilization for state values.</p>

<hr />

<p><font size=6>02/10: </font></p>

<p>In current research, the codomain of reward function on $\mathbb{R}$, how about $\mathbb{R} ^ n$ where $n \in \mathbb{N}$?</p>

<p>The state value is estimated by return (rewards sum after time $t$). However, we only have past and current rewards. How can we do this? How can this even be possible?</p>
        </article>
  </div>
</section>

<aside id=comments>
    <div><h2> Comments </h2></div>
    <div id="disqus_thread"></div>
<script>
    var disqus_config = function () {
    
    
    
    };
    (function() {
        if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
            document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
            return;
        }
        var d = document, s = d.createElement('script'); s.async = true;
        s.src = '//' + "blog-lvohfvy22n" + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
</aside>

<footer>
  <div>
    <p>
    &copy; 2018 <span itemprop="author" itemscope itemtype="http://schema.org/Person">
        <span itemprop="name">Qingfeng Lan.</span></span>
        Powered by <a href="http://hugo.spf13.com">Hugo</a>.
        Theme by <a href="http://spf13.com">Steve Francia</a>.
    </p>
  </div>
</footer>
<script type="text/javascript">
(function(){var j=function(a,b){return window.getComputedStyle?getComputedStyle(a).getPropertyValue(b):a.currentStyle[b]};var k=function(a,b,c){if(a.addEventListener)a.addEventListener(b,c,false);else a.attachEvent('on'+b,c)};var l=function(a,b){for(key in b)if(b.hasOwnProperty(key))a[key]=b[key];return a};window.fitText=function(d,e,f){var g=l({'minFontSize':-1/0,'maxFontSize':1/0},f);var h=function(a){var b=e||1;var c=function(){a.style.fontSize=Math.max(Math.min(a.clientWidth/(b*10),parseFloat(g.maxFontSize)),parseFloat(g.minFontSize))+'px'};c();k(window,'resize',c)};if(d.length)for(var i=0;i<d.length;i++)h(d[i]);else h(d);return d}})();
fitText(document.getElementById('title'), 1)
</script>

<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/8.4/styles/default.min.css">
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/8.4/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad();</script>

</body>
</html>

</body>