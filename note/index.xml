<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Notes on Qingfeng&#39;s blog</title>
    <link>https://lancelqf.github.io/note/</link>
    <description>Recent content in Notes on Qingfeng&#39;s blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>Copyright (c) 2018, Qingfeng Lan; all rights reserved.</copyright>
    <lastBuildDate>Tue, 01 Jan 2019 15:30:39 -0700</lastBuildDate>
    
	<atom:link href="https://lancelqf.github.io/note/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Reinforcement Learning - An Introduction</title>
      <link>https://lancelqf.github.io/note/reinforcement-learning-an-introduction/</link>
      <pubDate>Tue, 01 Jan 2019 15:30:39 -0700</pubDate>
      
      <guid>https://lancelqf.github.io/note/reinforcement-learning-an-introduction/</guid>
      <description>Reference  Reinforcement Learning: An Introduction, Richard S. Sutton and Andrew G. Barto, Second Edition, MIT Press, Cambridge, MA, 2018.  Notation    
Chapter 1: Introduction Reinforcement learning is a computational approach to understanding and automating goal-directed learning and decision making. It is distinguished from other computational approaches by its emphasis on learning by an agent from direct interaction with its environment, without requiring exemplary supervision or complete models of the environment.</description>
    </item>
    
    <item>
      <title>个人理财</title>
      <link>https://lancelqf.github.io/note/%E4%B8%AA%E4%BA%BA%E7%90%86%E8%B4%A2/</link>
      <pubDate>Mon, 24 Dec 2018 12:53:44 -0700</pubDate>
      
      <guid>https://lancelqf.github.io/note/%E4%B8%AA%E4%BA%BA%E7%90%86%E8%B4%A2/</guid>
      <description>&lt;p&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>test</title>
      <link>https://lancelqf.github.io/note/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B05_%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C1/</link>
      <pubDate>Sun, 01 Jan 2017 15:30:39 -0700</pubDate>
      
      <guid>https://lancelqf.github.io/note/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B05_%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C1/</guid>
      <description>&lt;p&gt;Andrew Ng cs229 Machine Learning 笔记&lt;/p&gt;

&lt;h1 id=&#34;神经网络&#34;&gt;神经网络&lt;/h1&gt;

&lt;h2 id=&#34;非线性假设&#34;&gt;非线性假设&lt;/h2&gt;

&lt;h2 id=&#34;2-1-a-k-armed-bandit-problem&#34;&gt;2.1 A k-armed Bandit Problem&lt;/h2&gt;

&lt;p&gt;In our k-armed bandit problem, each of the k actions has an expected or mean reward given that action is selected (i.e. the value of that action):&lt;/p&gt;

&lt;p&gt;$$q_{*}(a) \doteq \mathbb{E} [R_t|A_t = a]$$&lt;/p&gt;

&lt;p&gt;We denote the estimated value of action $a$ at time step $t$ as $Q_t(a)$. We would like $Q&lt;em&gt;t(a)$ to be close to $q&lt;/em&gt;{*}(a)$.&lt;/p&gt;

&lt;p&gt;在特征变量数较大的情况下，采用线性回归会很难处理，比如我的数据集有3个特征变量，想要在假设中引入所有特征变量的平方项：&lt;/p&gt;

&lt;div&gt;
$$g(\theta_0 + \theta_1x_1^2 + \theta_2x_1x_2 + \theta_3x_1x_3  + \theta_4x_2^2 + \theta_5x_2x_3  + \theta_6x_3^2 )$$
&lt;/div&gt;

&lt;p&gt;共有6个特征，假设我们想知道选取其中任意两个可重复的平方项有多少组合，采用允许重复的组合公式计算$\frac{(n+r-1)!}{r!(n-1)!}$，共有$\frac{(3 + 2 - 1)!}{(2!\cdot (3-1)!)} = 6$种特征变量的组合。对于100个特征变量，则共有$\frac{(100 + 2 - 1)!}{(2\cdot (100-1)!)} = 5050$个新的特征变量。&lt;/p&gt;

&lt;p&gt;可以大致估计特征变量的平方项组合个数的增长速度为$\mathcal{O}(\frac{n^2}2)$，立方项的组合个数的增长为$\mathcal{O}(n^3)$。这些增长都十分陡峭，让实际问题变得很棘手。&lt;/p&gt;

&lt;p&gt;在变量假设十分复杂的情况下，神经网络提供了另一种机器学习算法。
&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>